{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import json\n",
    "from datasets import list_datasets, load_dataset\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import word_tokenize\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    202990\n",
       "1    135739\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# subsets = ['Wireless_v1_00', 'Watches_v1_00', 'Video_Games_v1_00', 'Video_DVD_v1_00', 'Video_v1_00', 'Toys_v1_00', 'Tools_v1_00', 'Sports_v1_00', 'Software_v1_00', 'Shoes_v1_00', 'Pet_Products_v1_00', 'Personal_Care_Appliances_v1_00', 'PC_v1_00', 'Outdoors_v1_00', 'Office_Products_v1_00', 'Musical_Instruments_v1_00', 'Music_v1_00', 'Mobile_Electronics_v1_00', 'Mobile_Apps_v1_00', 'Major_Appliances_v1_00', 'Luggage_v1_00', 'Lawn_and_Garden_v1_00', 'Kitchen_v1_00', 'Jewelry_v1_00', 'Home_Improvement_v1_00', 'Home_Entertainment_v1_00', 'Home_v1_00', 'Health_Personal_Care_v1_00', 'Grocery_v1_00', 'Gift_Card_v1_00', 'Furniture_v1_00', 'Electronics_v1_00', 'Digital_Video_Games_v1_00', 'Digital_Video_Download_v1_00', 'Digital_Software_v1_00', 'Digital_Music_Purchase_v1_00', 'Digital_Ebook_Purchase_v1_00', 'Camera_v1_00', 'Books_v1_00', 'Beauty_v1_00', 'Baby_v1_00', 'Automotive_v1_00', 'Apparel_v1_00', 'Digital_Ebook_Purchase_v1_01']\n",
    "# results = []\n",
    "# for sub in subsets:\n",
    "#     print(sub)\n",
    "#     ds = load_dataset(\"amazon_us_reviews\", sub, split='train', streaming=True)\n",
    "#     results.extend(list(ds.take(20000)))\n",
    "old_df = pd.read_csv(\"data/pretrain/pretrain_data.csv\")\n",
    "old_df[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(each.strip() for each in (open(\"./stop_words.txt\").readlines()))\n",
    "stop_punct = [each.strip() for each in (open(\"./stop_punctaion.txt\").readlines())]\n",
    "def clean_text(text):\n",
    "    if pd.isna(text):\n",
    "        return text\n",
    "    return text.replace(r\"\\n\", \" \").replace(r\"<br />\", \" \").replace(\"&#34;\", \"\\\"\")\n",
    "def avg_token_len(l):\n",
    "    return np.mean(l.apply(lambda x:len(x.split())))\n",
    "\n",
    "def tokenize(content_list, stopwords, punct_pattern):\n",
    "    result = []\n",
    "    for s in tqdm(content_list):\n",
    "        if pd.isna(s):\n",
    "            continue\n",
    "        s = re.sub(punct_pattern, \"\", s)\n",
    "#         result.append([word for word in word_tokenize(s.lower()) if word not in stopwords])\n",
    "        result.append([word for word in s.lower().split() if word not in stopwords])\n",
    "    return result\n",
    "\n",
    "def word_count(text):\n",
    "    return len(text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation, NMF\n",
    "def lda_model(data, n_topics, n_top_words, n_jobs=1, method='lda', vectorizer='bow'):\n",
    "    \"\"\"\n",
    "    return: lda模型， 单词主题dataframe， 困惑度， 文档主题分布矩阵\n",
    "    \"\"\"\n",
    "    # 文档数*词汇表频率矩阵\n",
    "    assert method in ['lda', 'nmf']\n",
    "    assert vectorizer in ['bow', 'tfidf']\n",
    "    if vectorizer == \"bow\":\n",
    "        tf_vectorizer = CountVectorizer(max_df=0.9, min_df=2, stop_words='english', max_features=10000)\n",
    "    else:\n",
    "        tf_vectorizer = TfidfVectorizer(max_df=0.9, min_df=2, stop_words='english', max_features=10000)\n",
    "    countvector = tf_vectorizer.fit_transform(data)\n",
    "    # LDA模型\n",
    "    if method == 'lda':\n",
    "        lda = LatentDirichletAllocation(n_components=n_topics, max_iter=50, learning_method='batch', n_jobs=n_jobs,\n",
    "                                    random_state=10, batch_size=256)  # 变分推断EM\n",
    "    else:\n",
    "        lda = NMF(n_components=n_topics, max_iter=500, random_state=10) \n",
    "    docres = lda.fit_transform(countvector)\n",
    "    # 文档的主题分布\n",
    "    # 主题的词汇分布\n",
    "    feature_names = tf_vectorizer.get_feature_names()\n",
    "    res = pd.DataFrame()\n",
    "    for topic_idx, topic in enumerate(lda.components_):\n",
    "        # print('Topic %d' % topic_idx)\n",
    "        res[f'Topic_{topic_idx}'] = [feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]\n",
    "        # print(' '.join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    return lda, res, docres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### yahoo_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94e9f2d96d784d6889ac9036facc13f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/3.60k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdc321e3a0c14bb4b5cec53695f40510",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/1.88k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b54f40d0b4774b938200d49765ffcc1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/5.02k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset yahoo_answers_topics/yahoo_answers_topics to /home1/liumiao/.cache/huggingface/datasets/yahoo_answers_topics/yahoo_answers_topics/1.0.0/0edb353eefe79d9245d7bd7cac5ae6af19530439da520d6dde1c206ee38f4439...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e9a72e201804b7eb9dfaf48647b4ba2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/319M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/1400000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/60000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset yahoo_answers_topics downloaded and prepared to /home1/liumiao/.cache/huggingface/datasets/yahoo_answers_topics/yahoo_answers_topics/1.0.0/0edb353eefe79d9245d7bd7cac5ae6af19530439da520d6dde1c206ee38f4439. Subsequent calls will reuse this data.\n"
     ]
    }
   ],
   "source": [
    "# process yahoo_answer_data\n",
    "ds = load_dataset(\"yahoo_answers_topics\", split='train')\n",
    "TOPICS = [\n",
    "    \"Society and Culture\",\n",
    "    \"Science and Mathematics\",\n",
    "    \"Health\",\n",
    "    \"Education and Reference\",\n",
    "    \"Computers and Internet\",\n",
    "    \"Sports\",\n",
    "    \"Business and Finance\",\n",
    "    \"Entertainment and Music\",\n",
    "    \"Family and Relationships\",\n",
    "    \"Politics and Government\",\n",
    "]\n",
    "label_to_topic = {k: v for k, v in enumerate(TOPICS)}\n",
    "df = ds.to_pandas()\n",
    "df = df.replace(\"\", np.nan).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "part_df = df.sample(500000)[[\"topic\", \"question_title\", \"question_content\", \"best_answer\"]]\n",
    "remain_df = df.drop(index=part_df.index)\n",
    "remain_df.reset_index(inplace=True, drop=True)\n",
    "part_df.reset_index(inplace=True, drop=True)\n",
    "answer_pools = {k: remain_df[remain_df[\"topic\"]==k][\"best_answer\"].reset_index(drop=True) for k in range(len(TOPICS))}\n",
    "\n",
    "def gen_fake_answer(topic_id):\n",
    "    candidate_topics = list(range(10))\n",
    "    candidate_topics.pop(topic_id)\n",
    "    fake_topic = random.choice(candidate_topics)\n",
    "    fake_answer = random.choice(answer_pools[fake_topic])\n",
    "    return fake_answer\n",
    "\n",
    "part_df[\"fake_answer\"] = part_df[\"topic\"].apply(gen_fake_answer)\n",
    "part_df.columns = [\"topic_id\", \"title\", \"title_content\", \"answer\", \"fake_answer\"]\n",
    "part_df[\"topic_text\"] = part_df[\"topic_id\"].apply(lambda x: label_to_topic[x])\n",
    "# for each in [\"title\", \"title_content\", \"answer\"]:\n",
    "#     print(np.mean(part_df[each].apply(lambda x:len(x.split()))))\n",
    "candiate_col = [\"topic_text\", \"title\"]\n",
    "def random_concat(row):\n",
    "    text = \"\"\n",
    "    p = [random.randint(0, 1) for _ in range(len(candiate_col))]\n",
    "    for i in range(len(candiate_col)):\n",
    "        if p[i]:\n",
    "            text += row[candiate_col[i]]\n",
    "            if text[-1] not in stop_punct:\n",
    "                text += \".\"\n",
    "            text += \" \"\n",
    "    res = text.strip()\n",
    "    return res if len(res) else row[\"title\"]\n",
    "        \n",
    "    \n",
    "# part_df[\"topic\"] = part_df.apply(random_concat, axis=1)\n",
    "part_df[\"topic\"] = part_df[\"topic_text\"]\n",
    "part_df[\"topic\"] = part_df[\"topic\"].apply(clean_text)\n",
    "part_df[\"answer\"] = part_df[\"answer\"].apply(clean_text)\n",
    "part_df[\"fake_answer\"] = part_df[\"fake_answer\"].apply(clean_text)\n",
    "# part_df[\"topic\"] = part_df[\"topic_text\"] + \". \" + part_df[\"title\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "part_df = part_df[part_df.answer.apply(word_count) < 200].reset_index(drop=True)\n",
    "part_df = part_df[part_df.topic.apply(word_count) <20].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_index = np.array([True if random.random() <0.5 else False for _ in range(len(part_df))])\n",
    "pos = part_df[pos_index].copy()\n",
    "neg = part_df[~pos_index].copy()\n",
    "pos = pos[[\"topic\", \"answer\"]]\n",
    "pos.columns = [\"topic\", \"context\"]\n",
    "neg = neg[[\"topic\", \"fake_answer\"]]\n",
    "neg.columns = [\"topic\", \"context\"]\n",
    "pos[\"label\"] = 1\n",
    "pos = pos[pos[\"context\"].str.split().apply(lambda x: len(x)>3)]\n",
    "pos = pos.sample(50000)\n",
    "\n",
    "neg[\"label\"] = 0\n",
    "neg = neg[neg[\"context\"].str.split().apply(lambda x: len(x)>3)]\n",
    "neg = neg.sample(150000)\n",
    "yahoo_aug = pd.concat([pos, neg], ignore_index=True)\n",
    "\n",
    "\n",
    "# merge_df.to_csv(\"data/pretrain/yahoo_answer_aug_200000.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yahoo_aug.sample(frac=1).to_csv(\"data/pretrain/yahoo_answer_aug_1_3_30w.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DBpedia14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b39df6c3edd147f789bf3f0a2b84ada0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/5.22k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09b3f61cd4404f8e8ee394389d2e26ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/2.47k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af02be9a7d684b20b2da5cdbeddcd7f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/7.12k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset dbpedia_14/dbpedia_14 to /home1/liumiao/.cache/huggingface/datasets/dbpedia_14/dbpedia_14/2.0.0/01dab9e10d969eadcdbc918be5a09c9190a24caeae33b10eee8f367a1e3f1f0c...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3836a7a6caef4930a3ad62ca7783048a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/68.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe1ec8fa6eed47a0803a8c024b49b3ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/560000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61e960aade5d4add97d9e6f2e4775972",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/70000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset dbpedia_14 downloaded and prepared to /home1/liumiao/.cache/huggingface/datasets/dbpedia_14/dbpedia_14/2.0.0/01dab9e10d969eadcdbc918be5a09c9190a24caeae33b10eee8f367a1e3f1f0c. Subsequent calls will reuse this data.\n"
     ]
    }
   ],
   "source": [
    "# dbpedia_14\n",
    "ds_dbpedia = load_dataset(\"dbpedia_14\", split='train')\n",
    "df_db = ds_dbpedia.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [\n",
    "    \"Company\",\n",
    "    \"EducationalInstitution\",\n",
    "    \"Artist\",\n",
    "    \"Athlete\",\n",
    "    \"OfficeHolder\",\n",
    "    \"MeanOfTransportation\",\n",
    "    \"Building\",\n",
    "    \"NaturalPlace\",\n",
    "    \"Village\",\n",
    "    \"Animal\",\n",
    "    \"Plant\",\n",
    "    \"Album\",\n",
    "    \"Film\",\n",
    "    \"WrittenWork\"\n",
    "]\n",
    "label_to_cate_db14 = {k: v for k, v in enumerate(categories)}\n",
    "\n",
    "cate_dic = {\n",
    "    0:[\"institution created to conduct business\", [\"company\", \"corporation\", \"firm\", \"business\", \"commerce\"]],\n",
    "    1:[\"an institution dedicated to education\", [\"educational institution\", \"education\", \"school\", \"university\", \"college\", \"student\", \"teaching\"]],\n",
    "    2:[\"person whose creative work shows sensitivity and imagination\", [\"artist\", \"art\", \"singler\", \"writer\", \"drawer\", \"musician\"]],\n",
    "    3:[\"a person trained to compete in sports\", [\"athlete\", \"sports\", \"player\", \"sportsman\", \"ballplayer\", \"competition\"]],\n",
    "    4:[\"someone who is appointed or elected to an office and who holds a position of trust\", [\"officeholder\", \"politician\", \"party\", \"national\", \"governor\", \"election\"]],\n",
    "    5:[\"facility consisting of the means and equipment necessary for the movement of passengers or goods\", [\"transportation\", \"ship\", \"car\", \"railway\", \"aircraft\"]],\n",
    "    6: [\"a structure that has a roof and walls and stands in one place\", [\"building\", \"house\", \"build\", \"place\", \"location\"]],\n",
    "    7: [\"a place in the natural physical world including plants and animals and landscapes etc.\", [\"natural place\", \"river\", \"mountain\", \"lake\", \"sea\", \"nature\"]],\n",
    "    8: [\"a community of people smaller than a town\", [\"village\", \"small town\", \"countryside\", \"rural\"]],\n",
    "    9: [\"a living creature in nature characterized by voluntary movement\", [\"animal\", \"creature\", \"organism\", \"wild\"]],\n",
    "    10: [\"a living organism lacking the power of locomotion and movement, has flower and leaves\", [\"plant\", \"flower\", \"tree\", \"leaves\", \"grow in soil\"]],\n",
    "    11:[\"one or more music recordings issued together\", [\"album\", \"record\", \"music\", \"song\",\"studio\", \"band\", \"singer\"]],\n",
    "    12: [\"a form of entertainment that enacts a story by a sequence of images and video\", [\"film\", \"movie\", \"actor\", \"director\", \"directed\", \"drama\",\"screenplay\", \"story and plot\", \"role\", \"character\", \"theater\"]],\n",
    "    13: [\"a written work or composition that has been published or printed on paper or online\", [\"written work\", \"book\", \"author\", \"content\", \"journal\", \"publish\", \"novel\", \"fiction\", \"magazine\", \"newspaper\"]]\n",
    "}\n",
    "\n",
    "def words_augment(label):\n",
    "    topic_word = cate_dic[label][-1][0]\n",
    "    candidates = cate_dic[label][-1][1:]\n",
    "    n_aug_words = random.randint(1, len(candidates))\n",
    "    aug_words = [topic_word] + random.sample(candidates, n_aug_words)\n",
    "    return \" \".join(aug_words)\n",
    "\n",
    "def augment_dbpedia(df, n_pos=100000, n_neg=100000):\n",
    "    aug_type = np.random.randint(0, 3, len(df))\n",
    "    ori = df[aug_type==0].copy()\n",
    "    def_aug = df[aug_type==1].copy()\n",
    "    word_aug = df[aug_type==2].copy()\n",
    "    ori[\"topic\"] = ori[\"label\"].apply(lambda x: cate_dic[x][-1][0])\n",
    "    def_aug[\"topic\"] = def_aug[\"label\"].apply(lambda x: cate_dic[x][0])\n",
    "    word_aug[\"topic\"] = word_aug[\"label\"].apply(words_augment)\n",
    "    df_aug = pd.concat([ori, def_aug, word_aug], ignore_index=True)[[\"label\", \"topic\", \"content\"]]\n",
    "    df_aug.columns = [\"topic_id\", \"topic\", \"context\"]\n",
    "    pos = df_aug.sample(n_pos)\n",
    "    pos[\"label\"] = 1\n",
    "    neg = df_aug.drop(pos.index, axis=0)\n",
    "    neg_topics = {i: neg[neg[\"topic_id\"] != i][\"topic\"].values for i in range(len(cate_dic))} \n",
    "    neg[\"topic\"] = neg[\"topic_id\"].apply(lambda x: random.choice(neg_topics[x]))\n",
    "    neg[\"label\"] = 0\n",
    "    return pd.concat([pos.sample(n_pos), neg.sample(n_neg)], ignore_index=True)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbpedia_aug = augment_dbpedia(df_db, n_pos=50000, n_neg=150000).drop(\"topic_id\", axis=1)\n",
    "dbpedia_aug.sample(frac=1).to_csv(\"data/pretrain/dbpedia14_aug_1_3_20w.csv\", index=False)\n",
    "# dbpedia_aug = pd.read_csv(\"data/pretrain/dbpedia14_200000.csv\")\n",
    "# print(avg_token_len(out[\"topic\"]))\n",
    "# print(avg_token_len(out[\"context\"]))\n",
    "# out[\"topic_id\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_df = df_db[df_db[\"label\"] == 13]\n",
    "# tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "# countvector = tf_vectorizer.fit_transform(class_df[\"content\"])\n",
    "# model, tfidf_res, tfidf_docres = lda_model(df_db[\"content\"], 14, 10, n_jobs=-1, method='nmf', vectorizer='tfidf')\n",
    "model, class_res, class_docres = lda_model(class_df[\"content\"], 5, 20, n_jobs=-1, method='nmf', vectorizer='bow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### amazon_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon = pd.read_csv(\"data/amazon_review.csv\")\n",
    "amazon[\"review_body\"] = amazon[\"review_body\"].apply(clean_text)\n",
    "amazon = amazon[[\"product_title\", \"product_category\", \"star_rating\", \"review_headline\", \"review_body\"]]\n",
    "amazon.dropna(inplace=True)\n",
    "amazon = amazon[amazon[\"review_body\"].apply(word_count) < 200].copy().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_cate = ['Digital_Ebook_Purchase',\n",
    " 'Baby',\n",
    " 'Watches',\n",
    " 'Digital_Software',\n",
    " 'Jewelry',\n",
    " 'Personal_Care_Appliances',\n",
    " 'Music',\n",
    " 'Beauty',\n",
    " 'Pet Products',\n",
    " 'Office Products',\n",
    " 'Furniture',\n",
    " 'Camera',\n",
    " 'Major Appliances',\n",
    " 'Mobile_Electronics',\n",
    " 'Books',\n",
    " 'Automotive',\n",
    " 'Outdoors',\n",
    " 'PC',\n",
    " 'Apparel',\n",
    " 'Lawn and Garden',\n",
    " 'Mobile_Apps',\n",
    " 'Health & Personal Care',\n",
    " 'Grocery',\n",
    " 'Kitchen',\n",
    " 'Digital_Music_Purchase',\n",
    " 'Digital_Video_Download',\n",
    " 'Tools',\n",
    " 'Gift Card',\n",
    " 'Toys',\n",
    " 'Video',\n",
    " 'Software',\n",
    " 'Video Games',\n",
    " 'Electronics',\n",
    " 'Video DVD',\n",
    " 'Home Improvement',\n",
    " 'Musical Instruments',\n",
    " 'Sports',\n",
    " 'Wireless',\n",
    " 'Home',\n",
    " 'Home Entertainment',\n",
    " 'Luggage',\n",
    " 'Digital_Video_Games',\n",
    " 'Shoes'\n",
    "]\n",
    "\n",
    "amazon_idx_to_category = {k: v for k, v in enumerate(product_cate)}\n",
    "amazon_category_to_idx = {v: k for k, v in amazon_idx_to_category.items()}\n",
    "merge_category = {\n",
    "    \"book\": [\"Digital_Ebook_Purchase\", \"Books\"],\n",
    "    \"video\": [\"Digital_Video_Download\", \"Video\", \"Video DVD\"],\n",
    "    \"game and entertainment\": [\"Video Games\", \"Home Entertainment\", \"Digital_Video_Games\"],\n",
    "    \"software and app\": [\"Digital_Software\", \"Mobile_Apps\", \"Software\"],\n",
    "    \"household appliances and product\": [\"Furniture\", \"Major Appliances\", \"Kitchen\", \"Home\", \"Home Improvement\"],\n",
    "    \"health and personal care\": [\"Personal_Care_Appliances\", \"Health & Personal Care\"],\n",
    "    \"music and musical instruments\":[\"Music\", \"Digital_Music_Purchase\", \"Musical Instruments\"],\n",
    "    \"electronics product\":[\"Mobile_Electronics\", \"Electronics\", \"Wireless\"]\n",
    "}\n",
    "category_to_aggregated_category = dict()\n",
    "for k, v in merge_category.items():\n",
    "    for each in v:\n",
    "        category_to_aggregated_category[each] = k\n",
    "for oc in product_cate:\n",
    "    if oc not in category_to_aggregated_category:\n",
    "        category_to_aggregated_category[oc] = oc.replace(\"_\", \" \").lower()\n",
    "\n",
    "aggregated_category = np.array(list(set(category_to_aggregated_category.values())))\n",
    "cate_aug = json.load(open(\"data/pretrain/amazon_category_aug.json\"))\n",
    "aggregated_cate_aug = dict()\n",
    "for k, v in cate_aug.items():\n",
    "    ag_c = category_to_aggregated_category[k]\n",
    "    if ag_c not in aggregated_cate_aug:\n",
    "        aggregated_cate_aug[ag_c] = set(v)\n",
    "    else:\n",
    "        aggregated_cate_aug[ag_c] = aggregated_cate_aug[ag_c].union(set(v))\n",
    "aggregated_cate_aug = {k: list(v) for k, v in aggregated_cate_aug.items()}\n",
    "\n",
    "amazon[\"agg_category\"] = amazon[\"product_category\"].apply(lambda x: category_to_aggregated_category[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_rebalance = pd.concat([amazon[amazon[\"agg_category\"] == each].sample(19000) for each in aggregated_category], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# category relevant\n",
    "def amazon_topic_augment(category):\n",
    "    p = random.random()\n",
    "    if p <0.4:\n",
    "        return category\n",
    "    else:\n",
    "        candidates = aggregated_cate_aug[category]\n",
    "        n_aug_words = random.randint(0, min(len(candidates), 3))\n",
    "        aug_words = [category] + random.sample(candidates, n_aug_words)\n",
    "        return \" \".join(aug_words)   \n",
    "\n",
    "def category_relevant_aug(df, n_pos, n_neg):\n",
    "    df[\"context\"] = df.apply(lambda x: x[\"product_title\"] + \". \" + x[\"review_body\"] if random.random() > 0.5 \n",
    "                      else x[\"review_body\"] + \". \" + x[\"product_title\"], axis=1)\n",
    "    df[\"topic\"] = df[\"agg_category\"].apply(amazon_topic_augment)\n",
    "    pos = df.sample(n_pos)\n",
    "    pos[\"label\"] = 1\n",
    "    neg = df.drop(pos.index, axis=0).sample(n_neg)\n",
    "    neg_topics = {c: neg[neg[\"agg_category\"] != c][\"topic\"].values for c in aggregated_cate_aug} \n",
    "    neg[\"topic\"] = neg[\"agg_category\"].apply(lambda x: random.choice(neg_topics[x]))\n",
    "    neg[\"label\"] = 0\n",
    "    pos = pos[[\"agg_category\", \"topic\", \"context\", \"label\"]]\n",
    "    neg = neg[[\"agg_category\", \"topic\", \"context\", \"label\"]]\n",
    "    return pd.concat([pos, neg], ignore_index=True)\n",
    "\n",
    "amazon_cate_df = category_relevant_aug(amazon_rebalance.sample(200000), n_pos=25000, n_neg=75000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# amazon_cate_df.drop(\"agg_category\", axis=1).sample(frac=1).to_csv(\"data/pretrain/amazon_cate_1_3_10w.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentiment relevant\n",
    "senti_label_map = {\n",
    "    0: [\"negative\", \"bad\", \"worse\", \"terrible\", \"awful\", \"sucks\", \"useless\", \"disappoint\", \"frustrating\"],\n",
    "    1: [\"positive\", \"good\", \"nice\", \"awesome\", \"comfortable\", \"excellent\", \"lovely\", \"perfect\", \"like\"]\n",
    "}\n",
    "def sentiment_word_aug(senti_label):\n",
    "    candidates = senti_label_map[senti_label]\n",
    "    n = random.randint(1, len(candidates))\n",
    "    return \" \".join(random.sample(candidates, n))\n",
    "def sentiment_relevant_aug(df, n_pos, n_neg):\n",
    "    bad = df[df[\"star_rating\"] < 3].copy()\n",
    "    bad[\"senti_label\"] = 0\n",
    "    good = df[df[\"star_rating\"]>3].sample(len(bad))\n",
    "    good[\"senti_label\"] = 1\n",
    "    df = pd.concat([good, bad], ignore_index=True)\n",
    "    df[\"context\"] = df.apply(lambda x: x[\"review_headline\"] + \". \" + x[\"review_body\"] if random.random() > 0.5 \n",
    "                  else x[\"review_body\"] + \". \" + x[\"review_headline\"], axis=1)\n",
    "    pos = df.sample(n_pos)[[\"senti_label\", \"context\"]]\n",
    "    pos[\"topic\"] = pos[\"senti_label\"].apply(sentiment_word_aug)\n",
    "    pos[\"label\"] = 1\n",
    "    neg = df.drop(pos.index, axis=0).sample(n_neg)[[\"senti_label\", \"context\"]]\n",
    "    neg[\"topic\"] = neg[\"senti_label\"].apply(lambda x: sentiment_word_aug(1-x))\n",
    "    neg[\"label\"] = 0\n",
    "    return pd.concat([pos, neg], ignore_index=True)\n",
    "\n",
    "amazon_senti_df = sentiment_relevant_aug(amazon_rebalance, n_pos=25000, n_neg=75000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# category + sentiment relevant\n",
    "def sc_context_aug(row):\n",
    "    text_list = [row[\"product_title\"], row[\"review_headline\"], row[\"review_body\"]]\n",
    "    random.shuffle(text_list)\n",
    "    result = \"\"\n",
    "    for each in text_list:\n",
    "        result += each\n",
    "        if result[-1] not in stop_punct:\n",
    "            result += \".\"\n",
    "        result += \" \"\n",
    "    return result.strip()\n",
    "\n",
    "def sc_topic_neg_aug(row):\n",
    "    p = random.random()\n",
    "    true_category = amazon_topic_augment(row[\"agg_category\"])\n",
    "    true_senti = sentiment_word_aug(row[\"senti_label\"])\n",
    "    fake_category = amazon_topic_augment(random.choice(aggregated_category[aggregated_category!=row[\"agg_category\"]]))\n",
    "    fake_senti = sentiment_word_aug(1 - row[\"senti_label\"])\n",
    "    if p < 0.2:\n",
    "        topics = [fake_category, fake_senti]\n",
    "    elif 0.2 <= p < 0.6:\n",
    "        topics = [true_category, fake_senti]\n",
    "    else:\n",
    "        topics = [fake_category, true_senti]\n",
    "    random.shuffle(topics)\n",
    "    return \" \".join(topics)\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "def sc_topic_pos_aug(row):\n",
    "    topics = []\n",
    "    topics.append(amazon_topic_augment(row[\"agg_category\"]))\n",
    "    topics.append(sentiment_word_aug(row[\"senti_label\"]))\n",
    "    random.shuffle(topics)\n",
    "    return \" \".join(topics)\n",
    "    \n",
    "    \n",
    "    \n",
    "def sentiment_category_relevant_aug(df, n_pos, n_neg):\n",
    "    bad = df[df[\"star_rating\"] < 3].copy()\n",
    "    bad[\"senti_label\"] = 0\n",
    "    good = df[df[\"star_rating\"]>3].sample(len(bad))\n",
    "    good[\"senti_label\"] = 1    \n",
    "    df = pd.concat([good, bad], ignore_index=True)\n",
    "    df[\"context\"] = df.apply(sc_context_aug, axis=1)\n",
    "    pos = df.sample(n_pos)[[\"senti_label\", \"agg_category\", \"context\"]]\n",
    "    pos[\"topic\"] = pos.apply(sc_topic_pos_aug, axis=1)\n",
    "    pos[\"label\"] = 1\n",
    "    \n",
    "    neg = df.drop(pos.index, axis=0).sample(n_neg)[[\"senti_label\", \"agg_category\", \"context\"]]\n",
    "    neg[\"topic\"] = neg.apply(sc_topic_neg_aug, axis=1)\n",
    "    neg[\"label\"] = 0\n",
    "    return pd.concat([pos, neg], ignore_index=True)\n",
    "\n",
    "amazon_senti_cate_relevant_aug = sentiment_category_relevant_aug(amazon_rebalance, 25000, 75000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = amazon_cate_df[[\"topic\", \"context\", \"label\"]]\n",
    "b = amazon_senti_df[[\"topic\", \"context\", \"label\"]]\n",
    "c = amazon_senti_cate_relevant_aug[[\"topic\", \"context\", \"label\"]]\n",
    "amazon_aug = pd.concat([a, b, c], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# amazon_aug.sample(frac=1).to_csv(\"data/pretrain/amazon_aug_1_3_30w.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.sample(frac=1).to_csv(\"data/pretrain/amazon_cate_10w_shuf.csv\", index=False)\n",
    "b.sample(frac=1).to_csv(\"data/pretrain/amazon_senti_10w_shuf.csv\", index=False)\n",
    "b.sample(frac=1).to_csv(\"data/pretrain/amazon_cate_senti_10w_shuf.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37.45852\n",
      "7.67228\n",
      "5.510215\n"
     ]
    }
   ],
   "source": [
    "print(avg_token_len(a[\"context\"]))\n",
    "print(avg_token_len(a[\"topic\"]))\n",
    "print(avg_token_len(dbpedia_aug[\"topic\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46.085285\n",
      "5.510215\n"
     ]
    }
   ],
   "source": [
    "print(avg_token_len(dbpedia_aug[\"context\"]))\n",
    "print(avg_token_len(dbpedia_aug[\"topic\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yahoo_aug = pd.read_csv(\"data/pretrain/yahoo_answer_aug_notc_200000.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.5392763414970504"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.23 * np.log(0.23) + 0.77 * np.log(0.77)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_data = pd.concat([yahoo_aug, dbpedia_aug, amazon_aug], ignore_index=True).sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    525000\n",
       "1    175000\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrain_data.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_data.to_csv(\"data/pretrain/pretrain_1_3_70w.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500000"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# product_cate_aug = dict()\n",
    "# for c in product_cate:\n",
    "#     class_df = amazon[amazon[\"product_category\"] == c]\n",
    "#     model, res, docres = lda_model((class_df[\"product_title\"] + \" \" + class_df[\"review_body\"]).dropna(), 1, 20, n_jobs=-1, method='nmf', vectorizer='bow')\n",
    "#     product_cate_aug[c.replace(\"_\", \" \").lower()] = res[\"Topic_0\"].values.tolist()\n",
    "#     print(c)\n",
    "#     break\n",
    "len(pretrain_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_title</th>\n",
       "      <th>product_category</th>\n",
       "      <th>star_rating</th>\n",
       "      <th>review_headline</th>\n",
       "      <th>review_body</th>\n",
       "      <th>agg_category</th>\n",
       "      <th>context</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Selfie Stick Fiblastiq&amp;trade; Extendable Wirel...</td>\n",
       "      <td>Wireless</td>\n",
       "      <td>4</td>\n",
       "      <td>A fun little gadget</td>\n",
       "      <td>I’m embarrassed to admit that until recently, ...</td>\n",
       "      <td>electronics product</td>\n",
       "      <td>I’m embarrassed to admit that until recently, ...</td>\n",
       "      <td>electronics product cable transmitter usb wire...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tribe AB40 Water Resistant Sports Armband with...</td>\n",
       "      <td>Wireless</td>\n",
       "      <td>5</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>Fits iPhone 6 well</td>\n",
       "      <td>electronics product</td>\n",
       "      <td>Tribe AB40 Water Resistant Sports Armband with...</td>\n",
       "      <td>electronics product transmitter receiver reall...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RAVPower® Element 10400mAh External Battery US...</td>\n",
       "      <td>Wireless</td>\n",
       "      <td>5</td>\n",
       "      <td>Great charger</td>\n",
       "      <td>Great charger.  I easily get 3+ charges on a S...</td>\n",
       "      <td>electronics product</td>\n",
       "      <td>RAVPower® Element 10400mAh External Battery US...</td>\n",
       "      <td>electronics product battery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fosmon Micro USB Value Pack Bundle for Samsung...</td>\n",
       "      <td>Wireless</td>\n",
       "      <td>5</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>Great for the price :-)</td>\n",
       "      <td>electronics product</td>\n",
       "      <td>Great for the price :-). Fosmon Micro USB Valu...</td>\n",
       "      <td>electronics product pad battery receiver headp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>iPhone 6 Case, Vofolen Impact Resistant Protec...</td>\n",
       "      <td>Wireless</td>\n",
       "      <td>5</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>Great Case, better customer service!</td>\n",
       "      <td>electronics product</td>\n",
       "      <td>Great Case, better customer service!. iPhone 6...</td>\n",
       "      <td>electronics product wireless phone pad really ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>879992</th>\n",
       "      <td>Giving It to the Bad Boy (Tattooed and Pierced...</td>\n",
       "      <td>Digital_Ebook_Purchase</td>\n",
       "      <td>5</td>\n",
       "      <td>A Surprise Gem!</td>\n",
       "      <td>I am not a fan of the New Adult/Young Adult ge...</td>\n",
       "      <td>book</td>\n",
       "      <td>Giving It to the Bad Boy (Tattooed and Pierced...</td>\n",
       "      <td>book author fiction novel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>879993</th>\n",
       "      <td>One Night in Bridgeport</td>\n",
       "      <td>Digital_Ebook_Purchase</td>\n",
       "      <td>4</td>\n",
       "      <td>Loved the way it ended!!</td>\n",
       "      <td>Much to my surprise, I found myself getting so...</td>\n",
       "      <td>book</td>\n",
       "      <td>Much to my surprise, I found myself getting so...</td>\n",
       "      <td>book series novel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>879994</th>\n",
       "      <td>Takeover (Comet Clement series, #9)</td>\n",
       "      <td>Digital_Ebook_Purchase</td>\n",
       "      <td>4</td>\n",
       "      <td>Takeover 9 comet Clement series</td>\n",
       "      <td>Less format issues. I have a system now to mak...</td>\n",
       "      <td>book</td>\n",
       "      <td>Takeover (Comet Clement series, #9). Less form...</td>\n",
       "      <td>book author story series reading characters re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>879996</th>\n",
       "      <td>Pyromarne (The Heart of the Caveat Whale Book 2)</td>\n",
       "      <td>Digital_Ebook_Purchase</td>\n",
       "      <td>5</td>\n",
       "      <td>Pyromarne</td>\n",
       "      <td>This book was much better than the first book ...</td>\n",
       "      <td>book</td>\n",
       "      <td>This book was much better than the first book ...</td>\n",
       "      <td>book</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>879998</th>\n",
       "      <td>The Suiting: 25th Anniversary Edition (Horror ...</td>\n",
       "      <td>Digital_Ebook_Purchase</td>\n",
       "      <td>5</td>\n",
       "      <td>Wonderfully horrific!</td>\n",
       "      <td>Reb MacRath's The Suiting is a wonderfully hor...</td>\n",
       "      <td>book</td>\n",
       "      <td>Reb MacRath's The Suiting is a wonderfully hor...</td>\n",
       "      <td>book story fiction reading series author novel...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>696305 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            product_title  \\\n",
       "1       Selfie Stick Fiblastiq&trade; Extendable Wirel...   \n",
       "2       Tribe AB40 Water Resistant Sports Armband with...   \n",
       "3       RAVPower® Element 10400mAh External Battery US...   \n",
       "4       Fosmon Micro USB Value Pack Bundle for Samsung...   \n",
       "5       iPhone 6 Case, Vofolen Impact Resistant Protec...   \n",
       "...                                                   ...   \n",
       "879992  Giving It to the Bad Boy (Tattooed and Pierced...   \n",
       "879993                            One Night in Bridgeport   \n",
       "879994                Takeover (Comet Clement series, #9)   \n",
       "879996   Pyromarne (The Heart of the Caveat Whale Book 2)   \n",
       "879998  The Suiting: 25th Anniversary Edition (Horror ...   \n",
       "\n",
       "              product_category  star_rating                  review_headline  \\\n",
       "1                     Wireless            4              A fun little gadget   \n",
       "2                     Wireless            5                       Five Stars   \n",
       "3                     Wireless            5                    Great charger   \n",
       "4                     Wireless            5                       Five Stars   \n",
       "5                     Wireless            5                       Five Stars   \n",
       "...                        ...          ...                              ...   \n",
       "879992  Digital_Ebook_Purchase            5                  A Surprise Gem!   \n",
       "879993  Digital_Ebook_Purchase            4         Loved the way it ended!!   \n",
       "879994  Digital_Ebook_Purchase            4  Takeover 9 comet Clement series   \n",
       "879996  Digital_Ebook_Purchase            5                        Pyromarne   \n",
       "879998  Digital_Ebook_Purchase            5            Wonderfully horrific!   \n",
       "\n",
       "                                              review_body  \\\n",
       "1       I’m embarrassed to admit that until recently, ...   \n",
       "2                                      Fits iPhone 6 well   \n",
       "3       Great charger.  I easily get 3+ charges on a S...   \n",
       "4                                 Great for the price :-)   \n",
       "5                    Great Case, better customer service!   \n",
       "...                                                   ...   \n",
       "879992  I am not a fan of the New Adult/Young Adult ge...   \n",
       "879993  Much to my surprise, I found myself getting so...   \n",
       "879994  Less format issues. I have a system now to mak...   \n",
       "879996  This book was much better than the first book ...   \n",
       "879998  Reb MacRath's The Suiting is a wonderfully hor...   \n",
       "\n",
       "               agg_category  \\\n",
       "1       electronics product   \n",
       "2       electronics product   \n",
       "3       electronics product   \n",
       "4       electronics product   \n",
       "5       electronics product   \n",
       "...                     ...   \n",
       "879992                 book   \n",
       "879993                 book   \n",
       "879994                 book   \n",
       "879996                 book   \n",
       "879998                 book   \n",
       "\n",
       "                                                  context  \\\n",
       "1       I’m embarrassed to admit that until recently, ...   \n",
       "2       Tribe AB40 Water Resistant Sports Armband with...   \n",
       "3       RAVPower® Element 10400mAh External Battery US...   \n",
       "4       Great for the price :-). Fosmon Micro USB Valu...   \n",
       "5       Great Case, better customer service!. iPhone 6...   \n",
       "...                                                   ...   \n",
       "879992  Giving It to the Bad Boy (Tattooed and Pierced...   \n",
       "879993  Much to my surprise, I found myself getting so...   \n",
       "879994  Takeover (Comet Clement series, #9). Less form...   \n",
       "879996  This book was much better than the first book ...   \n",
       "879998  Reb MacRath's The Suiting is a wonderfully hor...   \n",
       "\n",
       "                                                    topic  \n",
       "1       electronics product cable transmitter usb wire...  \n",
       "2       electronics product transmitter receiver reall...  \n",
       "3                             electronics product battery  \n",
       "4       electronics product pad battery receiver headp...  \n",
       "5       electronics product wireless phone pad really ...  \n",
       "...                                                   ...  \n",
       "879992                          book author fiction novel  \n",
       "879993                                  book series novel  \n",
       "879994  book author story series reading characters re...  \n",
       "879996                                               book  \n",
       "879998  book story fiction reading series author novel...  \n",
       "\n",
       "[696305 rows x 8 columns]"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amazon[amazon[\"star_rating\"] >3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic_0</th>\n",
       "      <th>Topic_1</th>\n",
       "      <th>Topic_2</th>\n",
       "      <th>Topic_3</th>\n",
       "      <th>Topic_4</th>\n",
       "      <th>Topic_5</th>\n",
       "      <th>Topic_6</th>\n",
       "      <th>Topic_7</th>\n",
       "      <th>Topic_8</th>\n",
       "      <th>Topic_9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>stars</td>\n",
       "      <td>good</td>\n",
       "      <td>great</td>\n",
       "      <td>like</td>\n",
       "      <td>ok</td>\n",
       "      <td>nice</td>\n",
       "      <td>better</td>\n",
       "      <td>just</td>\n",
       "      <td>works</td>\n",
       "      <td>product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gave</td>\n",
       "      <td>quality</td>\n",
       "      <td>quality</td>\n",
       "      <td>really</td>\n",
       "      <td>price</td>\n",
       "      <td>looks</td>\n",
       "      <td>quality</td>\n",
       "      <td>okay</td>\n",
       "      <td>fine</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>given</td>\n",
       "      <td>price</td>\n",
       "      <td>price</td>\n",
       "      <td>don</td>\n",
       "      <td>game</td>\n",
       "      <td>small</td>\n",
       "      <td>price</td>\n",
       "      <td>fine</td>\n",
       "      <td>pretty</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>reason</td>\n",
       "      <td>pretty</td>\n",
       "      <td>looks</td>\n",
       "      <td>looks</td>\n",
       "      <td>just</td>\n",
       "      <td>quality</td>\n",
       "      <td>work</td>\n",
       "      <td>bad</td>\n",
       "      <td>use</td>\n",
       "      <td>use</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>giving</td>\n",
       "      <td>bad</td>\n",
       "      <td>worked</td>\n",
       "      <td>didn</td>\n",
       "      <td>read</td>\n",
       "      <td>looking</td>\n",
       "      <td>expected</td>\n",
       "      <td>small</td>\n",
       "      <td>little</td>\n",
       "      <td>disappointed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>fact</td>\n",
       "      <td>read</td>\n",
       "      <td>sound</td>\n",
       "      <td>look</td>\n",
       "      <td>guess</td>\n",
       "      <td>watch</td>\n",
       "      <td>little</td>\n",
       "      <td>work</td>\n",
       "      <td>doesn</td>\n",
       "      <td>fine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>original</td>\n",
       "      <td>sound</td>\n",
       "      <td>fit</td>\n",
       "      <td>doesn</td>\n",
       "      <td>book</td>\n",
       "      <td>price</td>\n",
       "      <td>needs</td>\n",
       "      <td>little</td>\n",
       "      <td>expected</td>\n",
       "      <td>does</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>super</td>\n",
       "      <td>looks</td>\n",
       "      <td>work</td>\n",
       "      <td>does</td>\n",
       "      <td>best</td>\n",
       "      <td>size</td>\n",
       "      <td>version</td>\n",
       "      <td>don</td>\n",
       "      <td>cheap</td>\n",
       "      <td>decent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>cover</td>\n",
       "      <td>book</td>\n",
       "      <td>game</td>\n",
       "      <td>feel</td>\n",
       "      <td>cheap</td>\n",
       "      <td>really</td>\n",
       "      <td>bad</td>\n",
       "      <td>really</td>\n",
       "      <td>time</td>\n",
       "      <td>work</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>time</td>\n",
       "      <td>game</td>\n",
       "      <td>idea</td>\n",
       "      <td>did</td>\n",
       "      <td>movie</td>\n",
       "      <td>little</td>\n",
       "      <td>thought</td>\n",
       "      <td>love</td>\n",
       "      <td>battery</td>\n",
       "      <td>did</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>lp</td>\n",
       "      <td>story</td>\n",
       "      <td>bad</td>\n",
       "      <td>size</td>\n",
       "      <td>light</td>\n",
       "      <td>look</td>\n",
       "      <td>use</td>\n",
       "      <td>didn</td>\n",
       "      <td>easy</td>\n",
       "      <td>really</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>collectors</td>\n",
       "      <td>value</td>\n",
       "      <td>little</td>\n",
       "      <td>picture</td>\n",
       "      <td>does</td>\n",
       "      <td>bag</td>\n",
       "      <td>fit</td>\n",
       "      <td>got</td>\n",
       "      <td>work</td>\n",
       "      <td>quality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>art</td>\n",
       "      <td>use</td>\n",
       "      <td>concept</td>\n",
       "      <td>use</td>\n",
       "      <td>use</td>\n",
       "      <td>color</td>\n",
       "      <td>way</td>\n",
       "      <td>cute</td>\n",
       "      <td>don</td>\n",
       "      <td>little</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>barely</td>\n",
       "      <td>really</td>\n",
       "      <td>bag</td>\n",
       "      <td>fit</td>\n",
       "      <td>app</td>\n",
       "      <td>pretty</td>\n",
       "      <td>liked</td>\n",
       "      <td>fit</td>\n",
       "      <td>sure</td>\n",
       "      <td>don</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>content</td>\n",
       "      <td>size</td>\n",
       "      <td>look</td>\n",
       "      <td>color</td>\n",
       "      <td>far</td>\n",
       "      <td>fit</td>\n",
       "      <td>wish</td>\n",
       "      <td>wish</td>\n",
       "      <td>advertised</td>\n",
       "      <td>time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>half</td>\n",
       "      <td>fit</td>\n",
       "      <td>watch</td>\n",
       "      <td>used</td>\n",
       "      <td>job</td>\n",
       "      <td>design</td>\n",
       "      <td>probably</td>\n",
       "      <td>does</td>\n",
       "      <td>perfect</td>\n",
       "      <td>didn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>maybe</td>\n",
       "      <td>light</td>\n",
       "      <td>size</td>\n",
       "      <td>lot</td>\n",
       "      <td>year</td>\n",
       "      <td>case</td>\n",
       "      <td>sound</td>\n",
       "      <td>bit</td>\n",
       "      <td>need</td>\n",
       "      <td>got</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>color</td>\n",
       "      <td>small</td>\n",
       "      <td>small</td>\n",
       "      <td>wanted</td>\n",
       "      <td>fit</td>\n",
       "      <td>cheap</td>\n",
       "      <td>ve</td>\n",
       "      <td>worked</td>\n",
       "      <td>does</td>\n",
       "      <td>need</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>worked</td>\n",
       "      <td>does</td>\n",
       "      <td>poor</td>\n",
       "      <td>pretty</td>\n",
       "      <td>expected</td>\n",
       "      <td>easy</td>\n",
       "      <td>think</td>\n",
       "      <td>disappointed</td>\n",
       "      <td>fit</td>\n",
       "      <td>best</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>rate</td>\n",
       "      <td>idea</td>\n",
       "      <td>case</td>\n",
       "      <td>game</td>\n",
       "      <td>money</td>\n",
       "      <td>doesn</td>\n",
       "      <td>buy</td>\n",
       "      <td>game</td>\n",
       "      <td>best</td>\n",
       "      <td>used</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Topic_0  Topic_1  Topic_2  Topic_3   Topic_4  Topic_5   Topic_6  \\\n",
       "0        stars     good    great     like        ok     nice    better   \n",
       "1         gave  quality  quality   really     price    looks   quality   \n",
       "2        given    price    price      don      game    small     price   \n",
       "3       reason   pretty    looks    looks      just  quality      work   \n",
       "4       giving      bad   worked     didn      read  looking  expected   \n",
       "5         fact     read    sound     look     guess    watch    little   \n",
       "6     original    sound      fit    doesn      book    price     needs   \n",
       "7        super    looks     work     does      best     size   version   \n",
       "8        cover     book     game     feel     cheap   really       bad   \n",
       "9         time     game     idea      did     movie   little   thought   \n",
       "10          lp    story      bad     size     light     look       use   \n",
       "11  collectors    value   little  picture      does      bag       fit   \n",
       "12         art      use  concept      use       use    color       way   \n",
       "13      barely   really      bag      fit       app   pretty     liked   \n",
       "14     content     size     look    color       far      fit      wish   \n",
       "15        half      fit    watch     used       job   design  probably   \n",
       "16       maybe    light     size      lot      year     case     sound   \n",
       "17       color    small    small   wanted       fit    cheap        ve   \n",
       "18      worked     does     poor   pretty  expected     easy     think   \n",
       "19        rate     idea     case     game     money    doesn       buy   \n",
       "\n",
       "         Topic_7     Topic_8       Topic_9  \n",
       "0           just       works       product  \n",
       "1           okay        fine          love  \n",
       "2           fine      pretty           bad  \n",
       "3            bad         use           use  \n",
       "4          small      little  disappointed  \n",
       "5           work       doesn          fine  \n",
       "6         little    expected          does  \n",
       "7            don       cheap        decent  \n",
       "8         really        time          work  \n",
       "9           love     battery           did  \n",
       "10          didn        easy        really  \n",
       "11           got        work       quality  \n",
       "12          cute         don        little  \n",
       "13           fit        sure           don  \n",
       "14          wish  advertised          time  \n",
       "15          does     perfect          didn  \n",
       "16           bit        need           got  \n",
       "17        worked        does          need  \n",
       "18  disappointed         fit          best  \n",
       "19          game        best          used  "
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_df = amazon[amazon[\"star_rating\"] == 3]\n",
    "model, res, docres = lda_model((class_df[\"review_headline\"]).dropna(), 10, 20, n_jobs=-1, method='nmf', vectorizer='bow')\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500000    Aketek 1080P LED Protable Projector HD PC AV V...\n",
       "500001               TiVo Mini with IR Remote (Old Version)\n",
       "500002    Apple TV MD199LL/A Bundle including remote and...\n",
       "500003               New Roku 3 6.5 Foot HDMI - Bundle - v1\n",
       "500004    Generic DVI-I Dual-Link (M) to 15-Pin VGA (F) ...\n",
       "                                ...                        \n",
       "519995    TICTID Powertv Newest and Most Professional Ar...\n",
       "519996    VIZIO E601i-A3 60-inch 1080p Razor LED Smart H...\n",
       "519997    BenQ HT1075 1080P 3D DLP Home Theater Projecto...\n",
       "519998       VIZIO E24-C1 E Series Class Razor LED Smart TV\n",
       "519999               New Roku 3 6.5 Foot HDMI - Bundle - v1\n",
       "Name: product_title, Length: 20000, dtype: object"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_df[\"product_title\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a100add787d248a187a7bab443b21b28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/880000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "pat = re.compile(r'[,.!?\\'\"()]')\n",
    "sentences = tokenize(amazon[\"product_title\"] + \" \" + amazon[\"review_body\"], stopwords, pat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = Word2Vec(sentences=sentences, vector_size=100, sg=1, hs=0, negative=5, workers=16, window=8, max_vocab_size=7000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('terrible', 0.7654217481613159),\n",
       " ('horrible', 0.7610396146774292),\n",
       " ('awful', 0.7588950395584106),\n",
       " ('worst', 0.7330824136734009),\n",
       " ('happened', 0.7127538919448853),\n",
       " ('sucks', 0.7082301378250122),\n",
       " ('basically', 0.7047510147094727),\n",
       " ('caused', 0.697221040725708),\n",
       " ('useless', 0.6885159611701965),\n",
       " ('disappointment', 0.679756760597229),\n",
       " ('luck', 0.6780253648757935),\n",
       " ('bad', 0.6755885481834412),\n",
       " ('constantly', 0.6664032936096191),\n",
       " ('disappointing', 0.6648461818695068),\n",
       " ('stuck', 0.65578293800354),\n",
       " ('avoid', 0.6516667008399963),\n",
       " ('eventually', 0.6492887139320374),\n",
       " ('happen', 0.6312088966369629),\n",
       " ('poor', 0.6267474889755249),\n",
       " ('frustrating', 0.6244062781333923)]"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.wv.similar_by_word('worse', topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_cate_aug = {k.replace(\"_\", \" \").lower(): v for k, v in product_cate_aug.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(product_cate_aug, open(\"data/pretrain/amazon_category_aug.json\", \"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33.30516103220644"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_token_len(class_df[\"review_body\"].dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Digital_Ebook_Purchase': 40000,\n",
       " 'Baby': 20000,\n",
       " 'Watches': 20000,\n",
       " 'Digital_Software': 20000,\n",
       " 'Jewelry': 20000,\n",
       " 'Personal_Care_Appliances': 20000,\n",
       " 'Music': 20000,\n",
       " 'Beauty': 20000,\n",
       " 'Pet Products': 20000,\n",
       " 'Office Products': 20000,\n",
       " 'Furniture': 20000,\n",
       " 'Camera': 20000,\n",
       " 'Major Appliances': 20000,\n",
       " 'Mobile_Electronics': 20000,\n",
       " 'Books': 20000,\n",
       " 'Automotive': 20000,\n",
       " 'Outdoors': 20000,\n",
       " 'PC': 20000,\n",
       " 'Apparel': 20000,\n",
       " 'Lawn and Garden': 20000,\n",
       " 'Mobile_Apps': 20000,\n",
       " 'Health & Personal Care': 20000,\n",
       " 'Grocery': 20000,\n",
       " 'Kitchen': 20000,\n",
       " 'Digital_Music_Purchase': 20000,\n",
       " 'Digital_Video_Download': 20000,\n",
       " 'Tools': 20000,\n",
       " 'Gift Card': 20000,\n",
       " 'Toys': 20000,\n",
       " 'Video': 20000,\n",
       " 'Software': 20000,\n",
       " 'Video Games': 20000,\n",
       " 'Electronics': 20000,\n",
       " 'Video DVD': 20000,\n",
       " 'Home Improvement': 20000,\n",
       " 'Musical Instruments': 20000,\n",
       " 'Sports': 20000,\n",
       " 'Wireless': 20000,\n",
       " 'Home': 20000,\n",
       " 'Home Entertainment': 20000,\n",
       " 'Luggage': 20000,\n",
       " 'Digital_Video_Games': 20000,\n",
       " 'Shoes': 20000}"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amazon.product_category.value_counts().to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yahoo = pd.read_csv(\"data/pretrain/yahoo_answer_190722.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(\"data/pretrain/amazon_reviews_879819.csv\")\n",
    "test_df = test_df.sample(200000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "person whose creative work shows sensitivity and imagination                           4937\n",
       "athlete                                                                                4918\n",
       "film                                                                                   4835\n",
       "a living creature in nature characterized by voluntary movement                        4828\n",
       "transportation                                                                         4824\n",
       "                                                                                       ... \n",
       "written work magazine author book novel fiction                                           1\n",
       "film story and plot screenplay drama theater role character director directed actor       1\n",
       "written work author content novel fiction journal newspaper magazine book                 1\n",
       "educational institution education teaching student college school university              1\n",
       "album song singer studio music band record                                                1\n",
       "Name: topic, Length: 11384, dtype: int64"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbpedia_aug[\"topic\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "awful disappoint sucks terrible negative worse bad\n",
      "So much for something my child can't throw... He had it off the table the first meal, and on the floor. The only benefit is that until he throws it, the food is held nicely/he doesn't move the bowl around the table trying to eat. So much for something my child can't throw.. ...\n",
      "1\n",
      "\n",
      "negative awful terrible\n",
      "Once you get a hang of the software it's pretty cool to design a building, but I can't create a basic driveway with the essentials edition? Weak.. Once you get a hang of the software it's pretty cool to design a building\n",
      "1\n",
      "\n",
      "frustrating worse sucks bad useless awful disappoint negative\n",
      "and while it wasn't a terrible difference, got more back on her refund. I've been using TurboTax for a few years now (SO has been using it for 10+ years) and never had any problems with it. I even decided to buy it despite the negative reviews on here, which I normally don't do. While it did what needed to be done for my taxes, it would NOT let me e-file my mom's state taxes while paper filing the federal one. I ended up re-entering all the information doing them on H&R Block's website anyway, and while it wasn't a terrible difference, got more back on her refund.  The bottomline is: if I'm paying $80 for this software, I should not have to use a competitor's website to finish doing my taxes! It's completely asinine. Sorry Intuit, but you're no longer the only game in town. Fix your software and listen to customers if you want to keep those who've been loyal to you for a decade. Next year I'm switching to H&R Block.\n",
      "1\n",
      "\n",
      "sucks awful useless worse negative frustrating bad terrible\n",
      "There is no way to replace just this glass as it is glued to the digitizer. I had to return this and buy the digitizer and touch screen glass as complete unit.. There is no way to replace just this glass as ...\n",
      "1\n",
      "\n",
      "like awesome nice positive lovely perfect comfortable good\n",
      "Five Stars. Great software love it !!!\n",
      "1\n",
      "\n",
      "sucks frustrating worse terrible\n",
      "Doesn't work all the time.. Not user friendly.  Doesn't work all the time. Purchased to use in hotels on vacation. I spent more time trying to get the box to find the input channel and gave up after thirty minutes. Worked one time and didn't the next day. Not worth the money. Going back to western digital.\n",
      "1\n",
      "\n",
      "perfect lovely positive excellent nice like comfortable\n",
      "loading works continuously and never comes up. I have .... loading works continuously and never comes up. I have tried in five home same result. unable to find the error. Joe Marvin\n",
      "0\n",
      "\n",
      "useless\n",
      "Horrible. Cheaply made. Definitely was not made for a Sony ZV3 as it does not fit. Maybe I got the wrong one? The headphones are ok. You get what you pay for.. Avoid.\n",
      "1\n",
      "\n",
      "good awesome\n",
      "This software is nothing but problems.  It slows down your computer to a crawl.  It doesn't  work for very long.  It's not working  a the moment.. This software is nothing but problems. It slows down ...\n",
      "0\n",
      "\n",
      "bad worse negative terrible frustrating sucks disappoint\n",
      "did nothing for me. Not helpful. Bulky.\n",
      "1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_s = b.sample(10)\n",
    "for idx, row in test_s.iterrows():\n",
    "    print(row[\"topic\"])\n",
    "    print(row[\"context\"])\n",
    "    print(row[\"label\"])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_review = pd.read_csv(\"data/pretrain/amazon_reviews_879819.csv\")\n",
    "amazon_review = amazon_review.sample(200000)\n",
    "amazon_review = amazon_review[amazon_review[\"context\"].str.split().apply(lambda x: len(x)>5)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_df = pd.concat([merge_df, amazon_review]).sample(frac=1)\n",
    "pretrain_df.to_csv(\"data/pretrain/pretrain_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = amazon_review[[\"review_body\", \"product_category\"]].copy()\n",
    "df.columns = [\"context\", \"topic\"]\n",
    "df[\"topic\"] = df[\"topic\"].apply(lambda x: x.lower().replace(\"_\", \" \"))\n",
    "\n",
    "# amazon_review.to_csv(\"data/amazon_review.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "pos_index = np.array([True if random.random() <0.3 else False for _ in range(len(df))])\n",
    "pos = df[pos_index].copy()\n",
    "neg = df[~pos_index].copy()\n",
    "pos[\"label\"] = 1\n",
    "neg[\"label\"] = 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = df[\"topic\"].unique()\n",
    "knowledge = {}\n",
    "\n",
    "for topic in topics:\n",
    "    if len(topic.split(\" \")) == 1:\n",
    "        synonyms = []\n",
    "        for syn in wordnet.synsets(topic):\n",
    "            for lm in syn.lemmas():\n",
    "                enrich = {'name': lm.name().replace(\"_\", \" \"), 'definition': syn.definition()}\n",
    "                if enrich not in synonyms:\n",
    "                    synonyms.append(enrich)#adding into synonyms\n",
    "        synonyms = synonyms[:5]\n",
    "        knowledge[topic] = synonyms\n",
    "    else:\n",
    "        knowledge[topic] = dict()\n",
    "        for each in topic.split(\" \"):\n",
    "            if len(each) > 1 and each != \"and\":\n",
    "                synonyms = []\n",
    "                for syn in wordnet.synsets(each):\n",
    "                    for lm in syn.lemmas():\n",
    "                        enrich = {'name': lm.name().replace(\"_\", \" \"), 'definition': syn.definition()}\n",
    "                        if enrich not in synonyms:\n",
    "                            synonyms.append(enrich)#adding into synonyms\n",
    "                synonyms = synonyms[:5]\n",
    "                knowledge[topic][each] = synonyms\n",
    "        \n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def topic_transform(topic):\n",
    "    p = random.random()\n",
    "    if p > 0.5:\n",
    "        return topic\n",
    "    \n",
    "    if p < 0.25:\n",
    "        # replace synonym\n",
    "        if isinstance(knowledge[topic], list):\n",
    "            l = knowledge[topic]\n",
    "            names = list(set(each['name'] for each in l if each['name'] != topic))\n",
    "            if len(names) == 0:\n",
    "                new_topic = topic\n",
    "            else:\n",
    "                new_topic = random.choice(names)\n",
    "        else:\n",
    "            d = knowledge[topic]\n",
    "            key = random.choice(list(d.keys()))\n",
    "            names = list(set(each['name'] for each in d[key] if each['name'] != topic))\n",
    "            if len(names) == 0:\n",
    "                new_topic = topic\n",
    "            else:\n",
    "                new_topic = topic.replace(key, random.choice(names))\n",
    "\n",
    "    else:\n",
    "        if isinstance(knowledge[topic], list):\n",
    "            defini = random.choice(knowledge[topic])['definition']\n",
    "            new_topic = topic + \" \" + defini\n",
    "        else:\n",
    "            d = knowledge[topic]\n",
    "            key = random.choice(list(d.keys()))\n",
    "            if len(d[key]) == 0:\n",
    "                new_topic = topic\n",
    "            else:\n",
    "                defini = random.choice(d[key])['definition']\n",
    "                new_topic = topic + \" \" + defini\n",
    "\n",
    "    return new_topic\n",
    "\n",
    "def gen_fake_topic(topic):\n",
    "    candidate = [each for each in knowledge.keys() if each != topic]\n",
    "    return random.choice(candidate)\n",
    "\n",
    "pos['topic'] = pos['topic'].apply(topic_transform)\n",
    "neg['topic'] = neg['topic'].apply(gen_fake_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = pd.concat([pos, neg])\n",
    "all_df = all_df.sort_index()\n",
    "all_df.to_csv(\"data/amazon_review_processed.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in knowledge.items():\n",
    "    if isinstance(v, dict):\n",
    "        if len(v.keys()) == 0:\n",
    "            print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"topic\"] = df[\"topic\"].apply(lambda x: x.lower().replace(\"_\", \" \"))\n",
    "df[\"topic\"].unique()\n",
    "df[\"label\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_datasets import PreTrainDataset\n",
    "from tqdm.notebook import tqdm\n",
    "pt = PreTrainDataset(\"data/amazon_review_processed.csv\", tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = tf.data.Dataset.from_generator(\n",
    "        pt.data_generator,\n",
    "        output_types=(tf.string, tf.string, tf.int32)\n",
    "    )\n",
    "\n",
    "loader = ds.batch(16).map(pt.wrap_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = iter(loader)\n",
    "next(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, each in tqdm(enumerate(loader), total=len(pt.data)/16):\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "ab1f4777d8eb8dfce46131a069ac753eeb3c9da22cbbba2710843f8e042f9b4a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
