{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "from datasets import list_datasets, load_dataset\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsets = ['Wireless_v1_00', 'Watches_v1_00', 'Video_Games_v1_00', 'Video_DVD_v1_00', 'Video_v1_00', 'Toys_v1_00', 'Tools_v1_00', 'Sports_v1_00', 'Software_v1_00', 'Shoes_v1_00', 'Pet_Products_v1_00', 'Personal_Care_Appliances_v1_00', 'PC_v1_00', 'Outdoors_v1_00', 'Office_Products_v1_00', 'Musical_Instruments_v1_00', 'Music_v1_00', 'Mobile_Electronics_v1_00', 'Mobile_Apps_v1_00', 'Major_Appliances_v1_00', 'Luggage_v1_00', 'Lawn_and_Garden_v1_00', 'Kitchen_v1_00', 'Jewelry_v1_00', 'Home_Improvement_v1_00', 'Home_Entertainment_v1_00', 'Home_v1_00', 'Health_Personal_Care_v1_00', 'Grocery_v1_00', 'Gift_Card_v1_00', 'Furniture_v1_00', 'Electronics_v1_00', 'Digital_Video_Games_v1_00', 'Digital_Video_Download_v1_00', 'Digital_Software_v1_00', 'Digital_Music_Purchase_v1_00', 'Digital_Ebook_Purchase_v1_00', 'Camera_v1_00', 'Books_v1_00', 'Beauty_v1_00', 'Baby_v1_00', 'Automotive_v1_00', 'Apparel_v1_00', 'Digital_Ebook_Purchase_v1_01']\n",
    "results = []\n",
    "for sub in subsets:\n",
    "    print(sub)\n",
    "    ds = load_dataset(\"amazon_us_reviews\", sub, split='train', streaming=True)\n",
    "    results.extend(list(ds.take(20000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process yahoo_answer_data\n",
    "ds = load_dataset(\"yahoo_answers_topics\", split='train')\n",
    "TOPICS = [\n",
    "    \"Society & Culture\",\n",
    "    \"Science & Mathematics\",\n",
    "    \"Health\",\n",
    "    \"Education & Reference\",\n",
    "    \"Computers & Internet\",\n",
    "    \"Sports\",\n",
    "    \"Business & Finance\",\n",
    "    \"Entertainment & Music\",\n",
    "    \"Family & Relationships\",\n",
    "    \"Politics & Government\",\n",
    "]\n",
    "label_to_topic = {k: v for k, v in enumerate(TOPICS)}\n",
    "df = ds.to_pandas()\n",
    "part_df = df.sample(200000)[[\"topic\", \"question_title\", \"best_answer\"]]\n",
    "remain_df = df.drop(index=part_df.index)\n",
    "remain_df.reset_index(inplace=True, drop=True)\n",
    "part_df.reset_index(inplace=True, drop=True)\n",
    "answer_pools = {k: remain_df[remain_df[\"topic\"]==k][\"best_answer\"].reset_index(drop=True) for k in range(len(TOPICS))}\n",
    "\n",
    "def gen_fake_answer(topic_id):\n",
    "    candidate_topics = list(range(10))\n",
    "    candidate_topics.pop(topic_id)\n",
    "    fake_topic = random.choice(candidate_topics)\n",
    "    fake_answer = random.choice(answer_pools[fake_topic])\n",
    "    return fake_answer\n",
    "\n",
    "part_df[\"fake_answer\"] = part_df[\"topic\"].apply(gen_fake_answer)\n",
    "part_df.columns = [\"topic_id\", \"title\", \"answer\", \"fake_answer\"]\n",
    "part_df[\"topic_text\"] = part_df[\"topic_id\"].apply(lambda x: label_to_topic[x])\n",
    "part_df[\"topic\"] = part_df[\"topic_text\"] + \". \" + part_df[\"title\"]\n",
    "\n",
    "pos_index = np.array([True if random.random() <0.5 else False for _ in range(len(part_df))])\n",
    "pos = part_df[pos_index].copy()\n",
    "neg = part_df[~pos_index].copy()\n",
    "pos = pos[[\"topic\", \"answer\"]]\n",
    "pos.columns = [\"topic\", \"context\"]\n",
    "neg = neg[[\"topic\", \"fake_answer\"]]\n",
    "neg.columns = [\"topic\", \"context\"]\n",
    "pos[\"label\"] = 1\n",
    "pos = pos[pos[\"context\"].str.split().apply(lambda x: len(x)>3)].copy()\n",
    "\n",
    "neg[\"label\"] = 0\n",
    "merge_df = pd.concat([pos, neg])\n",
    "\n",
    "def clean_text(text):\n",
    "    return text.replace(r\"\\n\", \" \").replace(r\"<br />\", \" \")\n",
    "merge_df[\"topic\"] = merge_df[\"topic\"].apply(clean_text)\n",
    "merge_df[\"context\"] = merge_df[\"context\"].apply(clean_text)\n",
    "\n",
    "# merge_df.to_csv(\"data/pretrain/yahoo_answer_190722.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_review = pd.read_csv(\"data/pretrain/amazon_reviews_879819.csv\")\n",
    "amazon_review = amazon_review.sample(200000)\n",
    "amazon_review = amazon_review[amazon_review[\"context\"].str.split().apply(lambda x: len(x)>5)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_df = pd.concat([merge_df, amazon_review]).sample(frac=1)\n",
    "pretrain_df.to_csv(\"data/pretrain/pretrain_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = amazon_review[[\"review_body\", \"product_category\"]].copy()\n",
    "df.columns = [\"context\", \"topic\"]\n",
    "df[\"topic\"] = df[\"topic\"].apply(lambda x: x.lower().replace(\"_\", \" \"))\n",
    "\n",
    "# amazon_review.to_csv(\"data/amazon_review.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "pos_index = np.array([True if random.random() <0.3 else False for _ in range(len(df))])\n",
    "pos = df[pos_index].copy()\n",
    "neg = df[~pos_index].copy()\n",
    "pos[\"label\"] = 1\n",
    "neg[\"label\"] = 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = df[\"topic\"].unique()\n",
    "knowledge = {}\n",
    "\n",
    "for topic in topics:\n",
    "    if len(topic.split(\" \")) == 1:\n",
    "        synonyms = []\n",
    "        for syn in wordnet.synsets(topic):\n",
    "            for lm in syn.lemmas():\n",
    "                enrich = {'name': lm.name().replace(\"_\", \" \"), 'definition': syn.definition()}\n",
    "                if enrich not in synonyms:\n",
    "                    synonyms.append(enrich)#adding into synonyms\n",
    "        synonyms = synonyms[:5]\n",
    "        knowledge[topic] = synonyms\n",
    "    else:\n",
    "        knowledge[topic] = dict()\n",
    "        for each in topic.split(\" \"):\n",
    "            if len(each) > 1 and each != \"and\":\n",
    "                synonyms = []\n",
    "                for syn in wordnet.synsets(each):\n",
    "                    for lm in syn.lemmas():\n",
    "                        enrich = {'name': lm.name().replace(\"_\", \" \"), 'definition': syn.definition()}\n",
    "                        if enrich not in synonyms:\n",
    "                            synonyms.append(enrich)#adding into synonyms\n",
    "                synonyms = synonyms[:5]\n",
    "                knowledge[topic][each] = synonyms\n",
    "        \n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def topic_transform(topic):\n",
    "    p = random.random()\n",
    "    if p > 0.5:\n",
    "        return topic\n",
    "    \n",
    "    if p < 0.25:\n",
    "        # replace synonym\n",
    "        if isinstance(knowledge[topic], list):\n",
    "            l = knowledge[topic]\n",
    "            names = list(set(each['name'] for each in l if each['name'] != topic))\n",
    "            if len(names) == 0:\n",
    "                new_topic = topic\n",
    "            else:\n",
    "                new_topic = random.choice(names)\n",
    "        else:\n",
    "            d = knowledge[topic]\n",
    "            key = random.choice(list(d.keys()))\n",
    "            names = list(set(each['name'] for each in d[key] if each['name'] != topic))\n",
    "            if len(names) == 0:\n",
    "                new_topic = topic\n",
    "            else:\n",
    "                new_topic = topic.replace(key, random.choice(names))\n",
    "\n",
    "    else:\n",
    "        if isinstance(knowledge[topic], list):\n",
    "            defini = random.choice(knowledge[topic])['definition']\n",
    "            new_topic = topic + \" \" + defini\n",
    "        else:\n",
    "            d = knowledge[topic]\n",
    "            key = random.choice(list(d.keys()))\n",
    "            if len(d[key]) == 0:\n",
    "                new_topic = topic\n",
    "            else:\n",
    "                defini = random.choice(d[key])['definition']\n",
    "                new_topic = topic + \" \" + defini\n",
    "\n",
    "    return new_topic\n",
    "\n",
    "def gen_fake_topic(topic):\n",
    "    candidate = [each for each in knowledge.keys() if each != topic]\n",
    "    return random.choice(candidate)\n",
    "\n",
    "pos['topic'] = pos['topic'].apply(topic_transform)\n",
    "neg['topic'] = neg['topic'].apply(gen_fake_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = pd.concat([pos, neg])\n",
    "all_df = all_df.sort_index()\n",
    "all_df.to_csv(\"data/amazon_review_processed.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in knowledge.items():\n",
    "    if isinstance(v, dict):\n",
    "        if len(v.keys()) == 0:\n",
    "            print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"topic\"] = df[\"topic\"].apply(lambda x: x.lower().replace(\"_\", \" \"))\n",
    "df[\"topic\"].unique()\n",
    "df[\"label\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_datasets import PreTrainDataset\n",
    "from tqdm.notebook import tqdm\n",
    "pt = PreTrainDataset(\"data/amazon_review_processed.csv\", tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = tf.data.Dataset.from_generator(\n",
    "        pt.data_generator,\n",
    "        output_types=(tf.string, tf.string, tf.int32)\n",
    "    )\n",
    "\n",
    "loader = ds.batch(16).map(pt.wrap_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = iter(loader)\n",
    "next(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, each in tqdm(enumerate(loader), total=len(pt.data)/16):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.data.Dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ab1f4777d8eb8dfce46131a069ac753eeb3c9da22cbbba2710843f8e042f9b4a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
