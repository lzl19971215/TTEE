{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import json\n",
    "from datasets import list_datasets, load_dataset\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import word_tokenize\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subsets = ['Wireless_v1_00', 'Watches_v1_00', 'Video_Games_v1_00', 'Video_DVD_v1_00', 'Video_v1_00', 'Toys_v1_00', 'Tools_v1_00', 'Sports_v1_00', 'Software_v1_00', 'Shoes_v1_00', 'Pet_Products_v1_00', 'Personal_Care_Appliances_v1_00', 'PC_v1_00', 'Outdoors_v1_00', 'Office_Products_v1_00', 'Musical_Instruments_v1_00', 'Music_v1_00', 'Mobile_Electronics_v1_00', 'Mobile_Apps_v1_00', 'Major_Appliances_v1_00', 'Luggage_v1_00', 'Lawn_and_Garden_v1_00', 'Kitchen_v1_00', 'Jewelry_v1_00', 'Home_Improvement_v1_00', 'Home_Entertainment_v1_00', 'Home_v1_00', 'Health_Personal_Care_v1_00', 'Grocery_v1_00', 'Gift_Card_v1_00', 'Furniture_v1_00', 'Electronics_v1_00', 'Digital_Video_Games_v1_00', 'Digital_Video_Download_v1_00', 'Digital_Software_v1_00', 'Digital_Music_Purchase_v1_00', 'Digital_Ebook_Purchase_v1_00', 'Camera_v1_00', 'Books_v1_00', 'Beauty_v1_00', 'Baby_v1_00', 'Automotive_v1_00', 'Apparel_v1_00', 'Digital_Ebook_Purchase_v1_01']\n",
    "# results = []\n",
    "# for sub in subsets:\n",
    "#     print(sub)\n",
    "#     ds = load_dataset(\"amazon_us_reviews\", sub, split='train', streaming=True)\n",
    "#     results.extend(list(ds.take(20000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(each.strip() for each in (open(\"./stop_words.txt\").readlines()))\n",
    "stop_punct = [each.strip() for each in (open(\"./stop_punctaion.txt\").readlines())]\n",
    "def clean_text(text):\n",
    "    if pd.isna(text):\n",
    "        return text\n",
    "    return text.replace(r\"\\n\", \" \").replace(r\"<br />\", \" \").replace(\"&#34;\", \"\\\"\")\n",
    "def avg_token_len(l):\n",
    "    return np.mean(l.apply(lambda x:len(x.split())))\n",
    "\n",
    "def tokenize(content_list, stopwords, punct_pattern):\n",
    "    result = []\n",
    "    for s in tqdm(content_list):\n",
    "        if pd.isna(s):\n",
    "            continue\n",
    "        s = re.sub(punct_pattern, \"\", s)\n",
    "#         result.append([word for word in word_tokenize(s.lower()) if word not in stopwords])\n",
    "        result.append([word for word in s.lower().split() if word not in stopwords])\n",
    "    return result\n",
    "\n",
    "def word_count(text):\n",
    "    return len(text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation, NMF\n",
    "def lda_model(data, n_topics, n_top_words, n_jobs=1, method='lda', vectorizer='bow'):\n",
    "    \"\"\"\n",
    "    return: lda模型， 单词主题dataframe， 困惑度， 文档主题分布矩阵\n",
    "    \"\"\"\n",
    "    # 文档数*词汇表频率矩阵\n",
    "    assert method in ['lda', 'nmf']\n",
    "    assert vectorizer in ['bow', 'tfidf']\n",
    "    if vectorizer == \"bow\":\n",
    "        tf_vectorizer = CountVectorizer(max_df=0.9, min_df=2, stop_words='english', max_features=10000)\n",
    "    else:\n",
    "        tf_vectorizer = TfidfVectorizer(max_df=0.9, min_df=2, stop_words='english', max_features=10000)\n",
    "    countvector = tf_vectorizer.fit_transform(data)\n",
    "    # LDA模型\n",
    "    if method == 'lda':\n",
    "        lda = LatentDirichletAllocation(n_components=n_topics, max_iter=50, learning_method='batch', n_jobs=n_jobs,\n",
    "                                    random_state=10, batch_size=256)  # 变分推断EM\n",
    "    else:\n",
    "        lda = NMF(n_components=n_topics, max_iter=500, random_state=10) \n",
    "    docres = lda.fit_transform(countvector)\n",
    "    # 文档的主题分布\n",
    "    # 主题的词汇分布\n",
    "    feature_names = tf_vectorizer.get_feature_names()\n",
    "    res = pd.DataFrame()\n",
    "    for topic_idx, topic in enumerate(lda.components_):\n",
    "        # print('Topic %d' % topic_idx)\n",
    "        res[f'Topic_{topic_idx}'] = [feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]\n",
    "        # print(' '.join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    return lda, res, docres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### yahoo_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94e9f2d96d784d6889ac9036facc13f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/3.60k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdc321e3a0c14bb4b5cec53695f40510",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/1.88k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b54f40d0b4774b938200d49765ffcc1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/5.02k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset yahoo_answers_topics/yahoo_answers_topics to /home1/liumiao/.cache/huggingface/datasets/yahoo_answers_topics/yahoo_answers_topics/1.0.0/0edb353eefe79d9245d7bd7cac5ae6af19530439da520d6dde1c206ee38f4439...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e9a72e201804b7eb9dfaf48647b4ba2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/319M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/1400000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/60000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset yahoo_answers_topics downloaded and prepared to /home1/liumiao/.cache/huggingface/datasets/yahoo_answers_topics/yahoo_answers_topics/1.0.0/0edb353eefe79d9245d7bd7cac5ae6af19530439da520d6dde1c206ee38f4439. Subsequent calls will reuse this data.\n"
     ]
    }
   ],
   "source": [
    "# process yahoo_answer_data\n",
    "ds = load_dataset(\"yahoo_answers_topics\", split='train')\n",
    "TOPICS = [\n",
    "    \"Society and Culture\",\n",
    "    \"Science and Mathematics\",\n",
    "    \"Health\",\n",
    "    \"Education and Reference\",\n",
    "    \"Computers and Internet\",\n",
    "    \"Sports\",\n",
    "    \"Business and Finance\",\n",
    "    \"Entertainment and Music\",\n",
    "    \"Family and Relationships\",\n",
    "    \"Politics and Government\",\n",
    "]\n",
    "label_to_topic = {k: v for k, v in enumerate(TOPICS)}\n",
    "df = ds.to_pandas()\n",
    "df = df.replace(\"\", np.nan).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>topic</th>\n",
       "      <th>question_title</th>\n",
       "      <th>question_content</th>\n",
       "      <th>best_answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>why doesn't an optical mouse work on a glass t...</td>\n",
       "      <td>or even on some surfaces?</td>\n",
       "      <td>Optical mice use an LED and a camera to rapidl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>What is the best off-road motorcycle trail ?</td>\n",
       "      <td>long-distance trail throughout CA</td>\n",
       "      <td>i hear that the mojave road is amazing!&lt;br /&gt;\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>What is Trans Fat? How to reduce that?</td>\n",
       "      <td>I heard that tras fat is bad for the body.  Wh...</td>\n",
       "      <td>Trans fats occur in manufactured foods during ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>How many planes Fedex has?</td>\n",
       "      <td>I heard that it is the largest airline in the ...</td>\n",
       "      <td>according to the www.fedex.com web site:\\nAir ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>In the san francisco bay area, does it make se...</td>\n",
       "      <td>the prices of rent and the price of buying doe...</td>\n",
       "      <td>renting vs buying depends on your goals. &lt;br /...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1399991</th>\n",
       "      <td>1399991</td>\n",
       "      <td>0</td>\n",
       "      <td>Why believe in such hopelessness?</td>\n",
       "      <td>Why believe in a religion, or lack there of, t...</td>\n",
       "      <td>Right on!!  I live a sheltered life, honestly....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1399992</th>\n",
       "      <td>1399992</td>\n",
       "      <td>3</td>\n",
       "      <td>where can i get a horse's skeletal and muscula...</td>\n",
       "      <td>mainly for a Breyer horse:Anatomy in Motion</td>\n",
       "      <td>Here's everything you can possibly wanna know....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1399993</th>\n",
       "      <td>1399993</td>\n",
       "      <td>9</td>\n",
       "      <td>In a quest to promote racial equality does the...</td>\n",
       "      <td>For example, by allowing jobs to advertise for...</td>\n",
       "      <td>Yes.  It also promotes gang culture in minorit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1399996</th>\n",
       "      <td>1399996</td>\n",
       "      <td>6</td>\n",
       "      <td>Ways to sell your video games?</td>\n",
       "      <td>Like if you want to sell your video games how ...</td>\n",
       "      <td>ebay, electronic boutique, babbages or flea ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1399998</th>\n",
       "      <td>1399998</td>\n",
       "      <td>0</td>\n",
       "      <td>Who can speak Hindi??</td>\n",
       "      <td>If you can write it here!!</td>\n",
       "      <td>Main hindi bol sakti hoon.kahiye.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>753677 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id  topic                                     question_title  \\\n",
       "0              0      4  why doesn't an optical mouse work on a glass t...   \n",
       "1              1      5       What is the best off-road motorcycle trail ?   \n",
       "2              2      2             What is Trans Fat? How to reduce that?   \n",
       "3              3      6                         How many planes Fedex has?   \n",
       "4              4      6  In the san francisco bay area, does it make se...   \n",
       "...          ...    ...                                                ...   \n",
       "1399991  1399991      0                  Why believe in such hopelessness?   \n",
       "1399992  1399992      3  where can i get a horse's skeletal and muscula...   \n",
       "1399993  1399993      9  In a quest to promote racial equality does the...   \n",
       "1399996  1399996      6                     Ways to sell your video games?   \n",
       "1399998  1399998      0                              Who can speak Hindi??   \n",
       "\n",
       "                                          question_content  \\\n",
       "0                                or even on some surfaces?   \n",
       "1                        long-distance trail throughout CA   \n",
       "2        I heard that tras fat is bad for the body.  Wh...   \n",
       "3        I heard that it is the largest airline in the ...   \n",
       "4        the prices of rent and the price of buying doe...   \n",
       "...                                                    ...   \n",
       "1399991  Why believe in a religion, or lack there of, t...   \n",
       "1399992        mainly for a Breyer horse:Anatomy in Motion   \n",
       "1399993  For example, by allowing jobs to advertise for...   \n",
       "1399996  Like if you want to sell your video games how ...   \n",
       "1399998                         If you can write it here!!   \n",
       "\n",
       "                                               best_answer  \n",
       "0        Optical mice use an LED and a camera to rapidl...  \n",
       "1        i hear that the mojave road is amazing!<br />\\...  \n",
       "2        Trans fats occur in manufactured foods during ...  \n",
       "3        according to the www.fedex.com web site:\\nAir ...  \n",
       "4        renting vs buying depends on your goals. <br /...  \n",
       "...                                                    ...  \n",
       "1399991  Right on!!  I live a sheltered life, honestly....  \n",
       "1399992  Here's everything you can possibly wanna know....  \n",
       "1399993  Yes.  It also promotes gang culture in minorit...  \n",
       "1399996  ebay, electronic boutique, babbages or flea ma...  \n",
       "1399998                  Main hindi bol sakti hoon.kahiye.  \n",
       "\n",
       "[753677 rows x 5 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "part_df = df.sample(300000)[[\"topic\", \"question_title\", \"question_content\", \"best_answer\"]]\n",
    "remain_df = df.drop(index=part_df.index)\n",
    "remain_df.reset_index(inplace=True, drop=True)\n",
    "part_df.reset_index(inplace=True, drop=True)\n",
    "answer_pools = {k: remain_df[remain_df[\"topic\"]==k][\"best_answer\"].reset_index(drop=True) for k in range(len(TOPICS))}\n",
    "\n",
    "def gen_fake_answer(topic_id):\n",
    "    candidate_topics = list(range(10))\n",
    "    candidate_topics.pop(topic_id)\n",
    "    fake_topic = random.choice(candidate_topics)\n",
    "    fake_answer = random.choice(answer_pools[fake_topic])\n",
    "    return fake_answer\n",
    "\n",
    "part_df[\"fake_answer\"] = part_df[\"topic\"].apply(gen_fake_answer)\n",
    "part_df.columns = [\"topic_id\", \"title\", \"title_content\", \"answer\", \"fake_answer\"]\n",
    "part_df[\"topic_text\"] = part_df[\"topic_id\"].apply(lambda x: label_to_topic[x])\n",
    "# for each in [\"title\", \"title_content\", \"answer\"]:\n",
    "#     print(np.mean(part_df[each].apply(lambda x:len(x.split()))))\n",
    "candiate_col = [\"topic_text\", \"title\", \"title_content\"]\n",
    "def random_concat(row):\n",
    "    text = \"\"\n",
    "    p = [random.randint(0, 1) for _ in range(len(candiate_col))]\n",
    "    for i in range(len(candiate_col)):\n",
    "        if p[i]:\n",
    "            text += row[candiate_col[i]]\n",
    "            text += \" \"\n",
    "    res = text.strip()\n",
    "    return res if len(res) else row[\"title\"]\n",
    "        \n",
    "    \n",
    "part_df[\"topic\"] = part_df.apply(random_concat, axis=1)\n",
    "part_df[\"topic\"] = part_df[\"topic\"].apply(clean_text)\n",
    "part_df[\"answer\"] = part_df[\"answer\"].apply(clean_text)\n",
    "part_df[\"fake_answer\"] = part_df[\"fake_answer\"].apply(clean_text)\n",
    "# part_df[\"topic\"] = part_df[\"topic_text\"] + \". \" + part_df[\"title\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_index = np.array([True if random.random() <0.5 else False for _ in range(len(part_df))])\n",
    "pos = part_df[pos_index].copy()\n",
    "neg = part_df[~pos_index].copy()\n",
    "pos = pos[[\"topic\", \"answer\"]]\n",
    "pos.columns = [\"topic\", \"context\"]\n",
    "neg = neg[[\"topic\", \"fake_answer\"]]\n",
    "neg.columns = [\"topic\", \"context\"]\n",
    "pos[\"label\"] = 1\n",
    "pos = pos[pos[\"context\"].str.split().apply(lambda x: len(x)>3)]\n",
    "pos = pos.sample(100000)\n",
    "\n",
    "neg[\"label\"] = 0\n",
    "neg = neg[neg[\"context\"].str.split().apply(lambda x: len(x)>3)]\n",
    "neg = neg.sample(100000)\n",
    "merge_df = pd.concat([pos, neg], ignore_index=True)\n",
    "\n",
    "\n",
    "# merge_df.to_csv(\"data/pretrain/yahoo_answer_aug_200000.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DBpedia14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset dbpedia_14 (/home1/liumiao/.cache/huggingface/datasets/dbpedia_14/dbpedia_14/2.0.0/01dab9e10d969eadcdbc918be5a09c9190a24caeae33b10eee8f367a1e3f1f0c)\n"
     ]
    }
   ],
   "source": [
    "# dbpedia_14\n",
    "ds_dbpedia = load_dataset(\"dbpedia_14\", split='train')\n",
    "df_db = ds_dbpedia.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [\n",
    "    \"Company\",\n",
    "    \"EducationalInstitution\",\n",
    "    \"Artist\",\n",
    "    \"Athlete\",\n",
    "    \"OfficeHolder\",\n",
    "    \"MeanOfTransportation\",\n",
    "    \"Building\",\n",
    "    \"NaturalPlace\",\n",
    "    \"Village\",\n",
    "    \"Animal\",\n",
    "    \"Plant\",\n",
    "    \"Album\",\n",
    "    \"Film\",\n",
    "    \"WrittenWork\"\n",
    "]\n",
    "label_to_cate_db14 = {k: v for k, v in enumerate(categories)}\n",
    "\n",
    "cate_dic = {\n",
    "    0:[\"institution created to conduct business\", [\"company\", \"corporation\", \"firm\", \"business\", \"commerce\"]],\n",
    "    1:[\"an institution dedicated to education\", [\"educational institution\", \"education\", \"school\", \"university\", \"college\", \"student\", \"teaching\"]],\n",
    "    2:[\"person whose creative work shows sensitivity and imagination\", [\"artist\", \"art\", \"singler\", \"writer\", \"drawer\", \"musician\"]],\n",
    "    3:[\"a person trained to compete in sports\", [\"athlete\", \"sports\", \"player\", \"sportsman\", \"ballplayer\", \"competition\"]],\n",
    "    4:[\"someone who is appointed or elected to an office and who holds a position of trust\", [\"officeholder\", \"politician\", \"party\", \"national\", \"governor\", \"election\"]],\n",
    "    5:[\"facility consisting of the means and equipment necessary for the movement of passengers or goods\", [\"transportation\", \"ship\", \"car\", \"railway\", \"aircraft\"]],\n",
    "    6: [\"a structure that has a roof and walls and stands in one place\", [\"building\", \"house\", \"build\", \"place\", \"location\"]],\n",
    "    7: [\"a place in the natural physical world including plants and animals and landscapes etc.\", [\"natural place\", \"river\", \"mountain\", \"lake\", \"sea\", \"nature\"]],\n",
    "    8: [\"a community of people smaller than a town\", [\"village\", \"small town\", \"countryside\", \"rural\"]],\n",
    "    9: [\"a living creature in nature characterized by voluntary movement\", [\"animal\", \"creature\", \"organism\", \"wild\"]],\n",
    "    10: [\"a living organism lacking the power of locomotion and movement, has flower and leaves\", [\"plant\", \"flower\", \"tree\", \"leaves\", \"grow in soil\"]],\n",
    "    11:[\"one or more music recordings issued together\", [\"album\", \"record\", \"music\", \"song\",\"studio\", \"band\", \"singer\"]],\n",
    "    12: [\"a form of entertainment that enacts a story by a sequence of images and video\", [\"film\", \"movie\", \"actor\", \"director\", \"directed\", \"drama\",\"screenplay\", \"story and plot\", \"role\", \"character\", \"theater\"]],\n",
    "    13: [\"a written work or composition that has been published or printed on paper or online\", [\"written work\", \"book\", \"author\", \"content\", \"journal\", \"publish\", \"novel\", \"fiction\", \"magazine\", \"newspaper\"]]\n",
    "}\n",
    "\n",
    "def words_augment(label):\n",
    "    topic_word = cate_dic[label][-1][0]\n",
    "    candidates = cate_dic[label][-1][1:]\n",
    "    n_aug_words = random.randint(1, len(candidates))\n",
    "    aug_words = [topic_word] + random.sample(candidates, n_aug_words)\n",
    "    return \" \".join(aug_words)\n",
    "\n",
    "def augment_dbpedia(df, n_pos=100000, n_neg=100000):\n",
    "    aug_type = np.random.randint(0, 3, len(df))\n",
    "    ori = df[aug_type==0].copy()\n",
    "    def_aug = df[aug_type==1].copy()\n",
    "    word_aug = df[aug_type==2].copy()\n",
    "    ori[\"topic\"] = ori[\"label\"].apply(lambda x: cate_dic[x][-1][0])\n",
    "    def_aug[\"topic\"] = def_aug[\"label\"].apply(lambda x: cate_dic[x][0])\n",
    "    word_aug[\"topic\"] = word_aug[\"label\"].apply(words_augment)\n",
    "    df_aug = pd.concat([ori, def_aug, word_aug], ignore_index=True)[[\"label\", \"topic\", \"content\"]]\n",
    "    df_aug.columns = [\"topic_id\", \"topic\", \"context\"]\n",
    "    pos = df_aug.sample(n_pos)\n",
    "    pos[\"label\"] = 1\n",
    "    neg = df_aug.drop(pos.index, axis=0)\n",
    "    neg_topics = {i: neg[neg[\"topic_id\"] != i][\"topic\"].values for i in range(len(cate_dic))} \n",
    "    neg[\"topic\"] = neg[\"topic_id\"].apply(lambda x: random.choice(neg_topics[x]))\n",
    "    neg[\"label\"] = 0\n",
    "    return pd.concat([pos.sample(n_pos), neg.sample(n_neg)], ignore_index=True)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.510215\n",
      "46.085285\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5     14523\n",
       "9     14390\n",
       "13    14334\n",
       "2     14315\n",
       "10    14311\n",
       "12    14289\n",
       "0     14286\n",
       "3     14282\n",
       "4     14250\n",
       "1     14246\n",
       "7     14235\n",
       "6     14234\n",
       "8     14222\n",
       "11    14083\n",
       "Name: topic_id, dtype: int64"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dbpedia_aug = augment_dbpedia(df_db)\n",
    "# dbpedia_aug(\"topic_id\", axis=1).to_csv(\"data/pretrain/dbpedia14_200000.csv\", index=False)\n",
    "dbpedia_aug = pd.read_csv(\"data/pretrain/dbpedia14_200000.csv\")\n",
    "print(avg_token_len(out[\"topic\"]))\n",
    "print(avg_token_len(out[\"context\"]))\n",
    "out[\"topic_id\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_df = df_db[df_db[\"label\"] == 13]\n",
    "# tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "# countvector = tf_vectorizer.fit_transform(class_df[\"content\"])\n",
    "# model, tfidf_res, tfidf_docres = lda_model(df_db[\"content\"], 14, 10, n_jobs=-1, method='nmf', vectorizer='tfidf')\n",
    "model, class_res, class_docres = lda_model(class_df[\"content\"], 5, 20, n_jobs=-1, method='nmf', vectorizer='bow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### amazon_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon = pd.read_csv(\"data/amazon_review.csv\")\n",
    "amazon[\"review_body\"] = amazon[\"review_body\"].apply(clean_text)\n",
    "amazon = amazon[[\"product_title\", \"product_category\", \"star_rating\", \"review_headline\", \"review_body\"]]\n",
    "amazon.dropna(inplace=True)\n",
    "amazon = amazon[amazon[\"review_body\"].apply(word_count) < 200].copy().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_cate = ['Digital_Ebook_Purchase',\n",
    " 'Baby',\n",
    " 'Watches',\n",
    " 'Digital_Software',\n",
    " 'Jewelry',\n",
    " 'Personal_Care_Appliances',\n",
    " 'Music',\n",
    " 'Beauty',\n",
    " 'Pet Products',\n",
    " 'Office Products',\n",
    " 'Furniture',\n",
    " 'Camera',\n",
    " 'Major Appliances',\n",
    " 'Mobile_Electronics',\n",
    " 'Books',\n",
    " 'Automotive',\n",
    " 'Outdoors',\n",
    " 'PC',\n",
    " 'Apparel',\n",
    " 'Lawn and Garden',\n",
    " 'Mobile_Apps',\n",
    " 'Health & Personal Care',\n",
    " 'Grocery',\n",
    " 'Kitchen',\n",
    " 'Digital_Music_Purchase',\n",
    " 'Digital_Video_Download',\n",
    " 'Tools',\n",
    " 'Gift Card',\n",
    " 'Toys',\n",
    " 'Video',\n",
    " 'Software',\n",
    " 'Video Games',\n",
    " 'Electronics',\n",
    " 'Video DVD',\n",
    " 'Home Improvement',\n",
    " 'Musical Instruments',\n",
    " 'Sports',\n",
    " 'Wireless',\n",
    " 'Home',\n",
    " 'Home Entertainment',\n",
    " 'Luggage',\n",
    " 'Digital_Video_Games',\n",
    " 'Shoes'\n",
    "]\n",
    "\n",
    "amazon_idx_to_category = {k: v for k, v in enumerate(product_cate)}\n",
    "amazon_category_to_idx = {v: k for k, v in amazon_idx_to_category.items()}\n",
    "merge_category = {\n",
    "    \"book\": [\"Digital_Ebook_Purchase\", \"Books\"],\n",
    "    \"video\": [\"Digital_Video_Download\", \"Video\", \"Video DVD\"],\n",
    "    \"game and entertainment\": [\"Video Games\", \"Home Entertainment\", \"Digital_Video_Games\"],\n",
    "    \"software and app\": [\"Digital_Software\", \"Mobile_Apps\", \"Software\"],\n",
    "    \"household appliances and product\": [\"Furniture\", \"Major Appliances\", \"Kitchen\", \"Home\", \"Home Improvement\"],\n",
    "    \"health and personal care\": [\"Personal_Care_Appliances\", \"Health & Personal Care\"],\n",
    "    \"music and musical instruments\":[\"Music\", \"Digital_Music_Purchase\", \"Musical Instruments\"],\n",
    "    \"electronics product\":[\"Mobile_Electronics\", \"Electronics\", \"Wireless\"]\n",
    "}\n",
    "category_to_aggregated_category = dict()\n",
    "for k, v in merge_category.items():\n",
    "    for each in v:\n",
    "        category_to_aggregated_category[each] = k\n",
    "for oc in product_cate:\n",
    "    if oc not in category_to_aggregated_category:\n",
    "        category_to_aggregated_category[oc] = oc.replace(\"_\", \" \").lower()\n",
    "\n",
    "aggregated_category = np.array(list(set(category_to_aggregated_category.values())))\n",
    "cate_aug = json.load(open(\"data/pretrain/amazon_category_aug.json\"))\n",
    "aggregated_cate_aug = dict()\n",
    "for k, v in cate_aug.items():\n",
    "    ag_c = category_to_aggregated_category[k]\n",
    "    if ag_c not in aggregated_cate_aug:\n",
    "        aggregated_cate_aug[ag_c] = set(v)\n",
    "    else:\n",
    "        aggregated_cate_aug[ag_c] = aggregated_cate_aug[ag_c].union(set(v))\n",
    "aggregated_cate_aug = {k: list(v) for k, v in aggregated_cate_aug.items()}\n",
    "\n",
    "amazon[\"agg_category\"] = amazon[\"product_category\"].apply(lambda x: category_to_aggregated_category[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# category relevant\n",
    "def amazon_topic_augment(category):\n",
    "    candidates = aggregated_cate_aug[category]\n",
    "    n_aug_words = random.randint(0, len(candidates))\n",
    "    aug_words = [category] + random.sample(candidates, n_aug_words)\n",
    "    return \" \".join(aug_words)   \n",
    "\n",
    "def category_relevant_aug(df, n_pos, n_neg):\n",
    "    df[\"context\"] = df.apply(lambda x: x[\"product_title\"] + \". \" + x[\"review_body\"] if random.random() > 0.5 \n",
    "                      else x[\"review_body\"] + \". \" + x[\"product_title\"], axis=1)\n",
    "    df[\"topic\"] = df[\"agg_category\"].apply(amazon_topic_augment)\n",
    "    pos = df.sample(n_pos)\n",
    "    pos[\"label\"] = 1\n",
    "    neg = df.drop(pos.index, axis=0).sample(n_neg)\n",
    "    neg_topics = {c: neg[neg[\"agg_category\"] != c][\"topic\"].values for c in aggregated_cate_aug} \n",
    "    neg[\"topic\"] = neg[\"agg_category\"].apply(lambda x: random.choice(neg_topics[x]))\n",
    "    neg[\"label\"] = 0\n",
    "    pos = pos[[\"agg_category\", \"topic\", \"context\", \"label\"]]\n",
    "    neg = neg[[\"agg_category\", \"topic\", \"context\", \"label\"]]\n",
    "    return pd.concat([pos, neg], ignore_index=True)\n",
    "\n",
    "amazon_cate_df = category_relevant_aug(amazon.sample(200000), n_pos=50000, n_neg=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentiment relevant\n",
    "senti_label_map = {\n",
    "    0: [\"negative\", \"bad\", \"worse\", \"terrible\", \"awful\", \"sucks\", \"useless\", \"disappoint\", \"frustrating\"],\n",
    "    1: [\"positive\", \"good\", \"nice\", \"awesome\", \"comfortable\", \"excellent\", \"lovely\", \"perfect\", \"like\"]\n",
    "}\n",
    "def sentiment_word_aug(senti_label):\n",
    "    candidates = senti_label_map[senti_label]\n",
    "    n = random.randint(1, len(candidates))\n",
    "    return \" \".join(random.sample(candidates, n))\n",
    "def sentiment_relevant_aug(df, n_pos, n_neg):\n",
    "    bad = df[df[\"star_rating\"] < 3].copy()\n",
    "    bad[\"senti_label\"] = 0\n",
    "    good = df[df[\"star_rating\"]>3].sample(len(bad))\n",
    "    good[\"senti_label\"] = 1\n",
    "    df = pd.concat([good, bad], ignore_index=True)\n",
    "    df[\"context\"] = df.apply(lambda x: x[\"review_headline\"] + \". \" + x[\"review_body\"] if random.random() > 0.5 \n",
    "                  else x[\"review_body\"] + \". \" + x[\"review_headline\"], axis=1)\n",
    "    pos = df.sample(n_pos)[[\"senti_label\", \"context\"]]\n",
    "    pos[\"topic\"] = pos[\"senti_label\"].apply(sentiment_word_aug)\n",
    "    pos[\"label\"] = 1\n",
    "    neg = df.drop(pos.index, axis=0).sample(n_neg)[[\"senti_label\", \"context\"]]\n",
    "    neg[\"topic\"] = neg[\"senti_label\"].apply(lambda x: sentiment_word_aug(1-x))\n",
    "    neg[\"label\"] = 0\n",
    "    return pd.concat([pos, neg], ignore_index=True)\n",
    "\n",
    "amazon_senti_df = sentiment_relevant_aug(amazon, n_pos=50000, n_neg=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# category + sentiment relevant\n",
    "def sc_context_aug(row):\n",
    "    text_list = [row[\"product_title\"], row[\"review_headline\"], row[\"review_body\"]]\n",
    "    random.shuffle(text_list)\n",
    "    result = \"\"\n",
    "    for each in text_list:\n",
    "        result += each\n",
    "        if result[-1] not in stop_punct:\n",
    "            result += \".\"\n",
    "        result += \" \"\n",
    "    return result.strip()\n",
    "\n",
    "def sc_topic_neg_aug(row):\n",
    "    p = random.random()\n",
    "    true_category = amazon_topic_augment(row[\"agg_category\"])\n",
    "    true_senti = sentiment_word_aug(row[\"senti_label\"])\n",
    "    fake_category = amazon_topic_augment(random.choice(aggregated_category[aggregated_category!=row[\"agg_category\"]]))\n",
    "    fake_senti = sentiment_word_aug(1 - row[\"senti_label\"])\n",
    "    if p < 0.2:\n",
    "        topics = [fake_category, fake_senti]\n",
    "    elif 0.2 <= p < 0.6:\n",
    "        topics = [true_category, fake_senti]\n",
    "    else:\n",
    "        topics = [fake_category, true_senti]\n",
    "    random.shuffle(topics)\n",
    "    return \" \".join(topics)\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "def sc_topic_pos_aug(row):\n",
    "    topics = []\n",
    "    topics.append(amazon_topic_augment(row[\"agg_category\"]))\n",
    "    topics.append(sentiment_word_aug(row[\"senti_label\"]))\n",
    "    random.shuffle(topics)\n",
    "    return \" \".join(topics)\n",
    "    \n",
    "    \n",
    "    \n",
    "def sentiment_category_relevant_aug(df, n_pos, n_neg):\n",
    "    bad = df[df[\"star_rating\"] < 3].copy()\n",
    "    bad[\"senti_label\"] = 0\n",
    "    good = df[df[\"star_rating\"]>3].sample(len(bad))\n",
    "    good[\"senti_label\"] = 1    \n",
    "    df = pd.concat([good, bad], ignore_index=True)\n",
    "    df[\"context\"] = df.apply(sc_context_aug, axis=1)\n",
    "    pos = df.sample(n_pos)[[\"senti_label\", \"agg_category\", \"context\"]]\n",
    "    pos[\"topic\"] = pos.apply(sc_topic_pos_aug, axis=1)\n",
    "    pos[\"label\"] = 1\n",
    "    \n",
    "    neg = df.drop(pos.index, axis=0).sample(n_neg)[[\"senti_label\", \"agg_category\", \"context\"]]\n",
    "    neg[\"topic\"] = neg.apply(sc_topic_neg_aug, axis=1)\n",
    "    neg[\"label\"] = 0\n",
    "    return pd.concat([pos, neg], ignore_index=True)\n",
    "\n",
    "amazon_senti_cate_relevant_aug = sentiment_category_relevant_aug(amazon, 50000, 50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = amazon_cate_df[[\"topic\", \"context\", \"label\"]]\n",
    "b = amazon_senti_df[[\"topic\", \"context\", \"label\"]]\n",
    "c = amazon_senti_cate_relevant_aug[[\"topic\", \"context\", \"label\"]]\n",
    "amazon_aug = pd.concat([a, b, c], ignore_index=True)\n",
    "amazon_aug.to_csv(\"data/pretrain/amazon_senti_cate_aug_300000\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_data = pd.concat([merge_df, dbpedia_aug, amazon_aug], ignore_index=True).sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_data.to_csv(\"data/pretrain/pretrain_data_700000.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>senti_label</th>\n",
       "      <th>topic</th>\n",
       "      <th>context</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>excellent good nice</td>\n",
       "      <td>i love it. Five Stars</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>awesome like perfect nice lovely good excellent</td>\n",
       "      <td>Easy to put on. Much easier than the barbed fi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>good positive like excellent perfect comfortab...</td>\n",
       "      <td>I have ordered the six inches wide and liked w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>bad disappoint</td>\n",
       "      <td>What I received was a complete joke, completel...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>perfect</td>\n",
       "      <td>Exactly as described, Fast Shipping and great ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>1</td>\n",
       "      <td>frustrating terrible worse bad</td>\n",
       "      <td>Makes life easier. Wife loves these</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>0</td>\n",
       "      <td>perfect excellent nice awesome like</td>\n",
       "      <td>Two Stars. Better if you know the source mater...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>0</td>\n",
       "      <td>good awesome</td>\n",
       "      <td>Waste of 8 dollars. This isn't even the origin...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>1</td>\n",
       "      <td>worse negative</td>\n",
       "      <td>Fond Memories. That was a wonderful night. I r...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>1</td>\n",
       "      <td>useless bad sucks frustrating terrible disappoint</td>\n",
       "      <td>Four Stars. Good coffee at a great price.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       senti_label                                              topic  \\\n",
       "0                1                                excellent good nice   \n",
       "1                1    awesome like perfect nice lovely good excellent   \n",
       "2                1  good positive like excellent perfect comfortab...   \n",
       "3                0                                     bad disappoint   \n",
       "4                1                                            perfect   \n",
       "...            ...                                                ...   \n",
       "99995            1                     frustrating terrible worse bad   \n",
       "99996            0                perfect excellent nice awesome like   \n",
       "99997            0                                       good awesome   \n",
       "99998            1                                     worse negative   \n",
       "99999            1  useless bad sucks frustrating terrible disappoint   \n",
       "\n",
       "                                                 context  label  \n",
       "0                                  i love it. Five Stars      1  \n",
       "1      Easy to put on. Much easier than the barbed fi...      1  \n",
       "2      I have ordered the six inches wide and liked w...      1  \n",
       "3      What I received was a complete joke, completel...      1  \n",
       "4      Exactly as described, Fast Shipping and great ...      1  \n",
       "...                                                  ...    ...  \n",
       "99995                Makes life easier. Wife loves these      0  \n",
       "99996  Two Stars. Better if you know the source mater...      0  \n",
       "99997  Waste of 8 dollars. This isn't even the origin...      0  \n",
       "99998  Fond Memories. That was a wonderful night. I r...      0  \n",
       "99999          Four Stars. Good coffee at a great price.      0  \n",
       "\n",
       "[100000 rows x 4 columns]"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# product_cate_aug = dict()\n",
    "# for c in product_cate:\n",
    "#     class_df = amazon[amazon[\"product_category\"] == c]\n",
    "#     model, res, docres = lda_model((class_df[\"product_title\"] + \" \" + class_df[\"review_body\"]).dropna(), 1, 20, n_jobs=-1, method='nmf', vectorizer='bow')\n",
    "#     product_cate_aug[c.replace(\"_\", \" \").lower()] = res[\"Topic_0\"].values.tolist()\n",
    "#     print(c)\n",
    "#     break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_title</th>\n",
       "      <th>product_category</th>\n",
       "      <th>star_rating</th>\n",
       "      <th>review_headline</th>\n",
       "      <th>review_body</th>\n",
       "      <th>agg_category</th>\n",
       "      <th>context</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Selfie Stick Fiblastiq&amp;trade; Extendable Wirel...</td>\n",
       "      <td>Wireless</td>\n",
       "      <td>4</td>\n",
       "      <td>A fun little gadget</td>\n",
       "      <td>I’m embarrassed to admit that until recently, ...</td>\n",
       "      <td>electronics product</td>\n",
       "      <td>I’m embarrassed to admit that until recently, ...</td>\n",
       "      <td>electronics product cable transmitter usb wire...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tribe AB40 Water Resistant Sports Armband with...</td>\n",
       "      <td>Wireless</td>\n",
       "      <td>5</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>Fits iPhone 6 well</td>\n",
       "      <td>electronics product</td>\n",
       "      <td>Tribe AB40 Water Resistant Sports Armband with...</td>\n",
       "      <td>electronics product transmitter receiver reall...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RAVPower® Element 10400mAh External Battery US...</td>\n",
       "      <td>Wireless</td>\n",
       "      <td>5</td>\n",
       "      <td>Great charger</td>\n",
       "      <td>Great charger.  I easily get 3+ charges on a S...</td>\n",
       "      <td>electronics product</td>\n",
       "      <td>RAVPower® Element 10400mAh External Battery US...</td>\n",
       "      <td>electronics product battery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fosmon Micro USB Value Pack Bundle for Samsung...</td>\n",
       "      <td>Wireless</td>\n",
       "      <td>5</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>Great for the price :-)</td>\n",
       "      <td>electronics product</td>\n",
       "      <td>Great for the price :-). Fosmon Micro USB Valu...</td>\n",
       "      <td>electronics product pad battery receiver headp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>iPhone 6 Case, Vofolen Impact Resistant Protec...</td>\n",
       "      <td>Wireless</td>\n",
       "      <td>5</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>Great Case, better customer service!</td>\n",
       "      <td>electronics product</td>\n",
       "      <td>Great Case, better customer service!. iPhone 6...</td>\n",
       "      <td>electronics product wireless phone pad really ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>879992</th>\n",
       "      <td>Giving It to the Bad Boy (Tattooed and Pierced...</td>\n",
       "      <td>Digital_Ebook_Purchase</td>\n",
       "      <td>5</td>\n",
       "      <td>A Surprise Gem!</td>\n",
       "      <td>I am not a fan of the New Adult/Young Adult ge...</td>\n",
       "      <td>book</td>\n",
       "      <td>Giving It to the Bad Boy (Tattooed and Pierced...</td>\n",
       "      <td>book author fiction novel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>879993</th>\n",
       "      <td>One Night in Bridgeport</td>\n",
       "      <td>Digital_Ebook_Purchase</td>\n",
       "      <td>4</td>\n",
       "      <td>Loved the way it ended!!</td>\n",
       "      <td>Much to my surprise, I found myself getting so...</td>\n",
       "      <td>book</td>\n",
       "      <td>Much to my surprise, I found myself getting so...</td>\n",
       "      <td>book series novel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>879994</th>\n",
       "      <td>Takeover (Comet Clement series, #9)</td>\n",
       "      <td>Digital_Ebook_Purchase</td>\n",
       "      <td>4</td>\n",
       "      <td>Takeover 9 comet Clement series</td>\n",
       "      <td>Less format issues. I have a system now to mak...</td>\n",
       "      <td>book</td>\n",
       "      <td>Takeover (Comet Clement series, #9). Less form...</td>\n",
       "      <td>book author story series reading characters re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>879996</th>\n",
       "      <td>Pyromarne (The Heart of the Caveat Whale Book 2)</td>\n",
       "      <td>Digital_Ebook_Purchase</td>\n",
       "      <td>5</td>\n",
       "      <td>Pyromarne</td>\n",
       "      <td>This book was much better than the first book ...</td>\n",
       "      <td>book</td>\n",
       "      <td>This book was much better than the first book ...</td>\n",
       "      <td>book</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>879998</th>\n",
       "      <td>The Suiting: 25th Anniversary Edition (Horror ...</td>\n",
       "      <td>Digital_Ebook_Purchase</td>\n",
       "      <td>5</td>\n",
       "      <td>Wonderfully horrific!</td>\n",
       "      <td>Reb MacRath's The Suiting is a wonderfully hor...</td>\n",
       "      <td>book</td>\n",
       "      <td>Reb MacRath's The Suiting is a wonderfully hor...</td>\n",
       "      <td>book story fiction reading series author novel...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>696305 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            product_title  \\\n",
       "1       Selfie Stick Fiblastiq&trade; Extendable Wirel...   \n",
       "2       Tribe AB40 Water Resistant Sports Armband with...   \n",
       "3       RAVPower® Element 10400mAh External Battery US...   \n",
       "4       Fosmon Micro USB Value Pack Bundle for Samsung...   \n",
       "5       iPhone 6 Case, Vofolen Impact Resistant Protec...   \n",
       "...                                                   ...   \n",
       "879992  Giving It to the Bad Boy (Tattooed and Pierced...   \n",
       "879993                            One Night in Bridgeport   \n",
       "879994                Takeover (Comet Clement series, #9)   \n",
       "879996   Pyromarne (The Heart of the Caveat Whale Book 2)   \n",
       "879998  The Suiting: 25th Anniversary Edition (Horror ...   \n",
       "\n",
       "              product_category  star_rating                  review_headline  \\\n",
       "1                     Wireless            4              A fun little gadget   \n",
       "2                     Wireless            5                       Five Stars   \n",
       "3                     Wireless            5                    Great charger   \n",
       "4                     Wireless            5                       Five Stars   \n",
       "5                     Wireless            5                       Five Stars   \n",
       "...                        ...          ...                              ...   \n",
       "879992  Digital_Ebook_Purchase            5                  A Surprise Gem!   \n",
       "879993  Digital_Ebook_Purchase            4         Loved the way it ended!!   \n",
       "879994  Digital_Ebook_Purchase            4  Takeover 9 comet Clement series   \n",
       "879996  Digital_Ebook_Purchase            5                        Pyromarne   \n",
       "879998  Digital_Ebook_Purchase            5            Wonderfully horrific!   \n",
       "\n",
       "                                              review_body  \\\n",
       "1       I’m embarrassed to admit that until recently, ...   \n",
       "2                                      Fits iPhone 6 well   \n",
       "3       Great charger.  I easily get 3+ charges on a S...   \n",
       "4                                 Great for the price :-)   \n",
       "5                    Great Case, better customer service!   \n",
       "...                                                   ...   \n",
       "879992  I am not a fan of the New Adult/Young Adult ge...   \n",
       "879993  Much to my surprise, I found myself getting so...   \n",
       "879994  Less format issues. I have a system now to mak...   \n",
       "879996  This book was much better than the first book ...   \n",
       "879998  Reb MacRath's The Suiting is a wonderfully hor...   \n",
       "\n",
       "               agg_category  \\\n",
       "1       electronics product   \n",
       "2       electronics product   \n",
       "3       electronics product   \n",
       "4       electronics product   \n",
       "5       electronics product   \n",
       "...                     ...   \n",
       "879992                 book   \n",
       "879993                 book   \n",
       "879994                 book   \n",
       "879996                 book   \n",
       "879998                 book   \n",
       "\n",
       "                                                  context  \\\n",
       "1       I’m embarrassed to admit that until recently, ...   \n",
       "2       Tribe AB40 Water Resistant Sports Armband with...   \n",
       "3       RAVPower® Element 10400mAh External Battery US...   \n",
       "4       Great for the price :-). Fosmon Micro USB Valu...   \n",
       "5       Great Case, better customer service!. iPhone 6...   \n",
       "...                                                   ...   \n",
       "879992  Giving It to the Bad Boy (Tattooed and Pierced...   \n",
       "879993  Much to my surprise, I found myself getting so...   \n",
       "879994  Takeover (Comet Clement series, #9). Less form...   \n",
       "879996  This book was much better than the first book ...   \n",
       "879998  Reb MacRath's The Suiting is a wonderfully hor...   \n",
       "\n",
       "                                                    topic  \n",
       "1       electronics product cable transmitter usb wire...  \n",
       "2       electronics product transmitter receiver reall...  \n",
       "3                             electronics product battery  \n",
       "4       electronics product pad battery receiver headp...  \n",
       "5       electronics product wireless phone pad really ...  \n",
       "...                                                   ...  \n",
       "879992                          book author fiction novel  \n",
       "879993                                  book series novel  \n",
       "879994  book author story series reading characters re...  \n",
       "879996                                               book  \n",
       "879998  book story fiction reading series author novel...  \n",
       "\n",
       "[696305 rows x 8 columns]"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amazon[amazon[\"star_rating\"] >3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic_0</th>\n",
       "      <th>Topic_1</th>\n",
       "      <th>Topic_2</th>\n",
       "      <th>Topic_3</th>\n",
       "      <th>Topic_4</th>\n",
       "      <th>Topic_5</th>\n",
       "      <th>Topic_6</th>\n",
       "      <th>Topic_7</th>\n",
       "      <th>Topic_8</th>\n",
       "      <th>Topic_9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>stars</td>\n",
       "      <td>good</td>\n",
       "      <td>great</td>\n",
       "      <td>like</td>\n",
       "      <td>ok</td>\n",
       "      <td>nice</td>\n",
       "      <td>better</td>\n",
       "      <td>just</td>\n",
       "      <td>works</td>\n",
       "      <td>product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gave</td>\n",
       "      <td>quality</td>\n",
       "      <td>quality</td>\n",
       "      <td>really</td>\n",
       "      <td>price</td>\n",
       "      <td>looks</td>\n",
       "      <td>quality</td>\n",
       "      <td>okay</td>\n",
       "      <td>fine</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>given</td>\n",
       "      <td>price</td>\n",
       "      <td>price</td>\n",
       "      <td>don</td>\n",
       "      <td>game</td>\n",
       "      <td>small</td>\n",
       "      <td>price</td>\n",
       "      <td>fine</td>\n",
       "      <td>pretty</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>reason</td>\n",
       "      <td>pretty</td>\n",
       "      <td>looks</td>\n",
       "      <td>looks</td>\n",
       "      <td>just</td>\n",
       "      <td>quality</td>\n",
       "      <td>work</td>\n",
       "      <td>bad</td>\n",
       "      <td>use</td>\n",
       "      <td>use</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>giving</td>\n",
       "      <td>bad</td>\n",
       "      <td>worked</td>\n",
       "      <td>didn</td>\n",
       "      <td>read</td>\n",
       "      <td>looking</td>\n",
       "      <td>expected</td>\n",
       "      <td>small</td>\n",
       "      <td>little</td>\n",
       "      <td>disappointed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>fact</td>\n",
       "      <td>read</td>\n",
       "      <td>sound</td>\n",
       "      <td>look</td>\n",
       "      <td>guess</td>\n",
       "      <td>watch</td>\n",
       "      <td>little</td>\n",
       "      <td>work</td>\n",
       "      <td>doesn</td>\n",
       "      <td>fine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>original</td>\n",
       "      <td>sound</td>\n",
       "      <td>fit</td>\n",
       "      <td>doesn</td>\n",
       "      <td>book</td>\n",
       "      <td>price</td>\n",
       "      <td>needs</td>\n",
       "      <td>little</td>\n",
       "      <td>expected</td>\n",
       "      <td>does</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>super</td>\n",
       "      <td>looks</td>\n",
       "      <td>work</td>\n",
       "      <td>does</td>\n",
       "      <td>best</td>\n",
       "      <td>size</td>\n",
       "      <td>version</td>\n",
       "      <td>don</td>\n",
       "      <td>cheap</td>\n",
       "      <td>decent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>cover</td>\n",
       "      <td>book</td>\n",
       "      <td>game</td>\n",
       "      <td>feel</td>\n",
       "      <td>cheap</td>\n",
       "      <td>really</td>\n",
       "      <td>bad</td>\n",
       "      <td>really</td>\n",
       "      <td>time</td>\n",
       "      <td>work</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>time</td>\n",
       "      <td>game</td>\n",
       "      <td>idea</td>\n",
       "      <td>did</td>\n",
       "      <td>movie</td>\n",
       "      <td>little</td>\n",
       "      <td>thought</td>\n",
       "      <td>love</td>\n",
       "      <td>battery</td>\n",
       "      <td>did</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>lp</td>\n",
       "      <td>story</td>\n",
       "      <td>bad</td>\n",
       "      <td>size</td>\n",
       "      <td>light</td>\n",
       "      <td>look</td>\n",
       "      <td>use</td>\n",
       "      <td>didn</td>\n",
       "      <td>easy</td>\n",
       "      <td>really</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>collectors</td>\n",
       "      <td>value</td>\n",
       "      <td>little</td>\n",
       "      <td>picture</td>\n",
       "      <td>does</td>\n",
       "      <td>bag</td>\n",
       "      <td>fit</td>\n",
       "      <td>got</td>\n",
       "      <td>work</td>\n",
       "      <td>quality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>art</td>\n",
       "      <td>use</td>\n",
       "      <td>concept</td>\n",
       "      <td>use</td>\n",
       "      <td>use</td>\n",
       "      <td>color</td>\n",
       "      <td>way</td>\n",
       "      <td>cute</td>\n",
       "      <td>don</td>\n",
       "      <td>little</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>barely</td>\n",
       "      <td>really</td>\n",
       "      <td>bag</td>\n",
       "      <td>fit</td>\n",
       "      <td>app</td>\n",
       "      <td>pretty</td>\n",
       "      <td>liked</td>\n",
       "      <td>fit</td>\n",
       "      <td>sure</td>\n",
       "      <td>don</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>content</td>\n",
       "      <td>size</td>\n",
       "      <td>look</td>\n",
       "      <td>color</td>\n",
       "      <td>far</td>\n",
       "      <td>fit</td>\n",
       "      <td>wish</td>\n",
       "      <td>wish</td>\n",
       "      <td>advertised</td>\n",
       "      <td>time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>half</td>\n",
       "      <td>fit</td>\n",
       "      <td>watch</td>\n",
       "      <td>used</td>\n",
       "      <td>job</td>\n",
       "      <td>design</td>\n",
       "      <td>probably</td>\n",
       "      <td>does</td>\n",
       "      <td>perfect</td>\n",
       "      <td>didn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>maybe</td>\n",
       "      <td>light</td>\n",
       "      <td>size</td>\n",
       "      <td>lot</td>\n",
       "      <td>year</td>\n",
       "      <td>case</td>\n",
       "      <td>sound</td>\n",
       "      <td>bit</td>\n",
       "      <td>need</td>\n",
       "      <td>got</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>color</td>\n",
       "      <td>small</td>\n",
       "      <td>small</td>\n",
       "      <td>wanted</td>\n",
       "      <td>fit</td>\n",
       "      <td>cheap</td>\n",
       "      <td>ve</td>\n",
       "      <td>worked</td>\n",
       "      <td>does</td>\n",
       "      <td>need</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>worked</td>\n",
       "      <td>does</td>\n",
       "      <td>poor</td>\n",
       "      <td>pretty</td>\n",
       "      <td>expected</td>\n",
       "      <td>easy</td>\n",
       "      <td>think</td>\n",
       "      <td>disappointed</td>\n",
       "      <td>fit</td>\n",
       "      <td>best</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>rate</td>\n",
       "      <td>idea</td>\n",
       "      <td>case</td>\n",
       "      <td>game</td>\n",
       "      <td>money</td>\n",
       "      <td>doesn</td>\n",
       "      <td>buy</td>\n",
       "      <td>game</td>\n",
       "      <td>best</td>\n",
       "      <td>used</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Topic_0  Topic_1  Topic_2  Topic_3   Topic_4  Topic_5   Topic_6  \\\n",
       "0        stars     good    great     like        ok     nice    better   \n",
       "1         gave  quality  quality   really     price    looks   quality   \n",
       "2        given    price    price      don      game    small     price   \n",
       "3       reason   pretty    looks    looks      just  quality      work   \n",
       "4       giving      bad   worked     didn      read  looking  expected   \n",
       "5         fact     read    sound     look     guess    watch    little   \n",
       "6     original    sound      fit    doesn      book    price     needs   \n",
       "7        super    looks     work     does      best     size   version   \n",
       "8        cover     book     game     feel     cheap   really       bad   \n",
       "9         time     game     idea      did     movie   little   thought   \n",
       "10          lp    story      bad     size     light     look       use   \n",
       "11  collectors    value   little  picture      does      bag       fit   \n",
       "12         art      use  concept      use       use    color       way   \n",
       "13      barely   really      bag      fit       app   pretty     liked   \n",
       "14     content     size     look    color       far      fit      wish   \n",
       "15        half      fit    watch     used       job   design  probably   \n",
       "16       maybe    light     size      lot      year     case     sound   \n",
       "17       color    small    small   wanted       fit    cheap        ve   \n",
       "18      worked     does     poor   pretty  expected     easy     think   \n",
       "19        rate     idea     case     game     money    doesn       buy   \n",
       "\n",
       "         Topic_7     Topic_8       Topic_9  \n",
       "0           just       works       product  \n",
       "1           okay        fine          love  \n",
       "2           fine      pretty           bad  \n",
       "3            bad         use           use  \n",
       "4          small      little  disappointed  \n",
       "5           work       doesn          fine  \n",
       "6         little    expected          does  \n",
       "7            don       cheap        decent  \n",
       "8         really        time          work  \n",
       "9           love     battery           did  \n",
       "10          didn        easy        really  \n",
       "11           got        work       quality  \n",
       "12          cute         don        little  \n",
       "13           fit        sure           don  \n",
       "14          wish  advertised          time  \n",
       "15          does     perfect          didn  \n",
       "16           bit        need           got  \n",
       "17        worked        does          need  \n",
       "18  disappointed         fit          best  \n",
       "19          game        best          used  "
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_df = amazon[amazon[\"star_rating\"] == 3]\n",
    "model, res, docres = lda_model((class_df[\"review_headline\"]).dropna(), 10, 20, n_jobs=-1, method='nmf', vectorizer='bow')\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500000    Aketek 1080P LED Protable Projector HD PC AV V...\n",
       "500001               TiVo Mini with IR Remote (Old Version)\n",
       "500002    Apple TV MD199LL/A Bundle including remote and...\n",
       "500003               New Roku 3 6.5 Foot HDMI - Bundle - v1\n",
       "500004    Generic DVI-I Dual-Link (M) to 15-Pin VGA (F) ...\n",
       "                                ...                        \n",
       "519995    TICTID Powertv Newest and Most Professional Ar...\n",
       "519996    VIZIO E601i-A3 60-inch 1080p Razor LED Smart H...\n",
       "519997    BenQ HT1075 1080P 3D DLP Home Theater Projecto...\n",
       "519998       VIZIO E24-C1 E Series Class Razor LED Smart TV\n",
       "519999               New Roku 3 6.5 Foot HDMI - Bundle - v1\n",
       "Name: product_title, Length: 20000, dtype: object"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_df[\"product_title\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a100add787d248a187a7bab443b21b28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/880000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "pat = re.compile(r'[,.!?\\'\"()]')\n",
    "sentences = tokenize(amazon[\"product_title\"] + \" \" + amazon[\"review_body\"], stopwords, pat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = Word2Vec(sentences=sentences, vector_size=100, sg=1, hs=0, negative=5, workers=16, window=8, max_vocab_size=7000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('terrible', 0.7654217481613159),\n",
       " ('horrible', 0.7610396146774292),\n",
       " ('awful', 0.7588950395584106),\n",
       " ('worst', 0.7330824136734009),\n",
       " ('happened', 0.7127538919448853),\n",
       " ('sucks', 0.7082301378250122),\n",
       " ('basically', 0.7047510147094727),\n",
       " ('caused', 0.697221040725708),\n",
       " ('useless', 0.6885159611701965),\n",
       " ('disappointment', 0.679756760597229),\n",
       " ('luck', 0.6780253648757935),\n",
       " ('bad', 0.6755885481834412),\n",
       " ('constantly', 0.6664032936096191),\n",
       " ('disappointing', 0.6648461818695068),\n",
       " ('stuck', 0.65578293800354),\n",
       " ('avoid', 0.6516667008399963),\n",
       " ('eventually', 0.6492887139320374),\n",
       " ('happen', 0.6312088966369629),\n",
       " ('poor', 0.6267474889755249),\n",
       " ('frustrating', 0.6244062781333923)]"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.wv.similar_by_word('worse', topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_cate_aug = {k.replace(\"_\", \" \").lower(): v for k, v in product_cate_aug.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(product_cate_aug, open(\"data/pretrain/amazon_category_aug.json\", \"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33.30516103220644"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_token_len(class_df[\"review_body\"].dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Digital_Ebook_Purchase': 40000,\n",
       " 'Baby': 20000,\n",
       " 'Watches': 20000,\n",
       " 'Digital_Software': 20000,\n",
       " 'Jewelry': 20000,\n",
       " 'Personal_Care_Appliances': 20000,\n",
       " 'Music': 20000,\n",
       " 'Beauty': 20000,\n",
       " 'Pet Products': 20000,\n",
       " 'Office Products': 20000,\n",
       " 'Furniture': 20000,\n",
       " 'Camera': 20000,\n",
       " 'Major Appliances': 20000,\n",
       " 'Mobile_Electronics': 20000,\n",
       " 'Books': 20000,\n",
       " 'Automotive': 20000,\n",
       " 'Outdoors': 20000,\n",
       " 'PC': 20000,\n",
       " 'Apparel': 20000,\n",
       " 'Lawn and Garden': 20000,\n",
       " 'Mobile_Apps': 20000,\n",
       " 'Health & Personal Care': 20000,\n",
       " 'Grocery': 20000,\n",
       " 'Kitchen': 20000,\n",
       " 'Digital_Music_Purchase': 20000,\n",
       " 'Digital_Video_Download': 20000,\n",
       " 'Tools': 20000,\n",
       " 'Gift Card': 20000,\n",
       " 'Toys': 20000,\n",
       " 'Video': 20000,\n",
       " 'Software': 20000,\n",
       " 'Video Games': 20000,\n",
       " 'Electronics': 20000,\n",
       " 'Video DVD': 20000,\n",
       " 'Home Improvement': 20000,\n",
       " 'Musical Instruments': 20000,\n",
       " 'Sports': 20000,\n",
       " 'Wireless': 20000,\n",
       " 'Home': 20000,\n",
       " 'Home Entertainment': 20000,\n",
       " 'Luggage': 20000,\n",
       " 'Digital_Video_Games': 20000,\n",
       " 'Shoes': 20000}"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amazon.product_category.value_counts().to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yahoo = pd.read_csv(\"data/pretrain/yahoo_answer_190722.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_review = pd.read_csv(\"data/pretrain/amazon_reviews_879819.csv\")\n",
    "amazon_review = amazon_review.sample(200000)\n",
    "amazon_review = amazon_review[amazon_review[\"context\"].str.split().apply(lambda x: len(x)>5)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_df = pd.concat([merge_df, amazon_review]).sample(frac=1)\n",
    "pretrain_df.to_csv(\"data/pretrain/pretrain_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = amazon_review[[\"review_body\", \"product_category\"]].copy()\n",
    "df.columns = [\"context\", \"topic\"]\n",
    "df[\"topic\"] = df[\"topic\"].apply(lambda x: x.lower().replace(\"_\", \" \"))\n",
    "\n",
    "# amazon_review.to_csv(\"data/amazon_review.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "pos_index = np.array([True if random.random() <0.3 else False for _ in range(len(df))])\n",
    "pos = df[pos_index].copy()\n",
    "neg = df[~pos_index].copy()\n",
    "pos[\"label\"] = 1\n",
    "neg[\"label\"] = 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = df[\"topic\"].unique()\n",
    "knowledge = {}\n",
    "\n",
    "for topic in topics:\n",
    "    if len(topic.split(\" \")) == 1:\n",
    "        synonyms = []\n",
    "        for syn in wordnet.synsets(topic):\n",
    "            for lm in syn.lemmas():\n",
    "                enrich = {'name': lm.name().replace(\"_\", \" \"), 'definition': syn.definition()}\n",
    "                if enrich not in synonyms:\n",
    "                    synonyms.append(enrich)#adding into synonyms\n",
    "        synonyms = synonyms[:5]\n",
    "        knowledge[topic] = synonyms\n",
    "    else:\n",
    "        knowledge[topic] = dict()\n",
    "        for each in topic.split(\" \"):\n",
    "            if len(each) > 1 and each != \"and\":\n",
    "                synonyms = []\n",
    "                for syn in wordnet.synsets(each):\n",
    "                    for lm in syn.lemmas():\n",
    "                        enrich = {'name': lm.name().replace(\"_\", \" \"), 'definition': syn.definition()}\n",
    "                        if enrich not in synonyms:\n",
    "                            synonyms.append(enrich)#adding into synonyms\n",
    "                synonyms = synonyms[:5]\n",
    "                knowledge[topic][each] = synonyms\n",
    "        \n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def topic_transform(topic):\n",
    "    p = random.random()\n",
    "    if p > 0.5:\n",
    "        return topic\n",
    "    \n",
    "    if p < 0.25:\n",
    "        # replace synonym\n",
    "        if isinstance(knowledge[topic], list):\n",
    "            l = knowledge[topic]\n",
    "            names = list(set(each['name'] for each in l if each['name'] != topic))\n",
    "            if len(names) == 0:\n",
    "                new_topic = topic\n",
    "            else:\n",
    "                new_topic = random.choice(names)\n",
    "        else:\n",
    "            d = knowledge[topic]\n",
    "            key = random.choice(list(d.keys()))\n",
    "            names = list(set(each['name'] for each in d[key] if each['name'] != topic))\n",
    "            if len(names) == 0:\n",
    "                new_topic = topic\n",
    "            else:\n",
    "                new_topic = topic.replace(key, random.choice(names))\n",
    "\n",
    "    else:\n",
    "        if isinstance(knowledge[topic], list):\n",
    "            defini = random.choice(knowledge[topic])['definition']\n",
    "            new_topic = topic + \" \" + defini\n",
    "        else:\n",
    "            d = knowledge[topic]\n",
    "            key = random.choice(list(d.keys()))\n",
    "            if len(d[key]) == 0:\n",
    "                new_topic = topic\n",
    "            else:\n",
    "                defini = random.choice(d[key])['definition']\n",
    "                new_topic = topic + \" \" + defini\n",
    "\n",
    "    return new_topic\n",
    "\n",
    "def gen_fake_topic(topic):\n",
    "    candidate = [each for each in knowledge.keys() if each != topic]\n",
    "    return random.choice(candidate)\n",
    "\n",
    "pos['topic'] = pos['topic'].apply(topic_transform)\n",
    "neg['topic'] = neg['topic'].apply(gen_fake_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = pd.concat([pos, neg])\n",
    "all_df = all_df.sort_index()\n",
    "all_df.to_csv(\"data/amazon_review_processed.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in knowledge.items():\n",
    "    if isinstance(v, dict):\n",
    "        if len(v.keys()) == 0:\n",
    "            print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"topic\"] = df[\"topic\"].apply(lambda x: x.lower().replace(\"_\", \" \"))\n",
    "df[\"topic\"].unique()\n",
    "df[\"label\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_datasets import PreTrainDataset\n",
    "from tqdm.notebook import tqdm\n",
    "pt = PreTrainDataset(\"data/amazon_review_processed.csv\", tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = tf.data.Dataset.from_generator(\n",
    "        pt.data_generator,\n",
    "        output_types=(tf.string, tf.string, tf.int32)\n",
    "    )\n",
    "\n",
    "loader = ds.batch(16).map(pt.wrap_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = iter(loader)\n",
    "next(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, each in tqdm(enumerate(loader), total=len(pt.data)/16):\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "ab1f4777d8eb8dfce46131a069ac753eeb3c9da22cbbba2710843f8e042f9b4a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
