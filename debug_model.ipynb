{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "import logging\n",
    "import json\n",
    "import tensorflow as tf\n",
    "# gpus = tf.config.experimental.list_physical_devices(device_type='GPU')\n",
    "# tf.config.experimental.set_virtual_device_configuration(gpus[0],[tf.config.experimental.VirtualDeviceConfiguration(memory_limit=2048)])\n",
    "from train import ABSATrainer\n",
    "from my_datasets import EnglishDataset, TestTokenizer, ChineseDataset\n",
    "from transformers import AutoTokenizer, TFAutoModel\n",
    "from label_mappings import *\n",
    "from utils.eval_utils import quick_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7f8ac47c4460>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7f8ac47c4460>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f892c26bbb0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from models.end_to_end_model import End2EndAspectSentimentModel\n",
    "model_checkpoint = \"checkpoint/ttee_pretrain_20000steps_0d_2e-5lr_2000_0.96_schedule_0cased_gate_0.3dropout_1loss/\"\n",
    "ckpt_path = tf.train.latest_checkpoint(model_checkpoint)\n",
    "config_path = os.path.join(model_checkpoint, \"model_config.json\")\n",
    "config = json.load(open(config_path))\n",
    "\n",
    "model = End2EndAspectSentimentModel.from_config(config)\n",
    "model.build_params()\n",
    "ckpt = tf.train.Checkpoint(model=model)\n",
    "ckpt.restore(ckpt_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-42cfff251026>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_tokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTestTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mASPECT_SENTENCE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_sb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "test_tokenizer = TestTokenizer(tokenizer, ASPECT_SENTENCE, mask_sb, model_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"end2_end_aspect_sentiment_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " tf_bert_model (TFBertModel)  multiple                 109482240 \n",
      "                                                                 \n",
      " fuse_net (FuseNet)          multiple                  1181184   \n",
      "                                                                 \n",
      " target_extraction_block (Ta  multiple                 2322      \n",
      " rgetExtractionBlock)                                            \n",
      "                                                                 \n",
      " tf_bert_attention (TFBertAt  multiple                 2363904   \n",
      " tention)                                                        \n",
      "                                                                 \n",
      " sequential_1 (Sequential)   (36, 1)                   769       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 113,058,068\n",
      "Trainable params: 113,030,419\n",
      "Non-trainable params: 27,649\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.te_block.trainable = True\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-cased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from models.end_to_end_model import End2EndAspectSentimentModel\n",
    "\n",
    "init_dir = 'bert_models/bert-base-cased'\n",
    "init_model = 'bert-base-cased'\n",
    "# file_path = '../data/semeval2016/ABSA16_Restaurants_Train_SB1_v2.xml'\n",
    "# tokenizer = AutoTokenizer.from_pretrained('bert-base-cased', cache_dir=init_dir)\n",
    "# tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "# dataset = SemEvalDataSet(file_path, tokenizer, sentence_b=ASPECT_SENTENCE, model_type=\"end_to_end\")\n",
    "# ds = tf.data.Dataset.from_generator(\n",
    "#     dataset.generate_string_sample,\n",
    "#     output_types=(tf.string, tf.string)\n",
    "# ).batch(batch_size=8).map(dataset.wrap_map)\n",
    "# model = End2EndAspectSentimentModel(init_bert_model=init_model, sentence_b=ASPECT_SENTENCE, cache_dir=init_dir, hot_attention=True, fuse_strategy=\"gate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_inputs = tf.ones([1,10,768])\n",
    "model.self_attention(dummy_inputs, attention_mask=None, head_mask=None, output_attentions=False)\n",
    "model.self_attention.get_weights()\n",
    "model.self_attention.set_weights(model.bert.bert.encoder.layer[-1].attention.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[[ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         ...,\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True]],\n",
       " \n",
       "        [[ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         ...,\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True]],\n",
       " \n",
       "        [[ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         ...,\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         ...,\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True]],\n",
       " \n",
       "        [[ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         ...,\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True]],\n",
       " \n",
       "        [[ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         ...,\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True]]]),\n",
       " array([[ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True]]),\n",
       " array([[[ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         ...,\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True]],\n",
       " \n",
       "        [[ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         ...,\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True]],\n",
       " \n",
       "        [[ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         ...,\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         ...,\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True]],\n",
       " \n",
       "        [[ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         ...,\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True]],\n",
       " \n",
       "        [[ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         ...,\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True]]]),\n",
       " array([[ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True]]),\n",
       " array([[[ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         ...,\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True]],\n",
       " \n",
       "        [[ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         ...,\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True]],\n",
       " \n",
       "        [[ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         ...,\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         ...,\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True]],\n",
       " \n",
       "        [[ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         ...,\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True]],\n",
       " \n",
       "        [[ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         ...,\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True]]]),\n",
       " array([[ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True]]),\n",
       " array([[[ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         ...,\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True]],\n",
       " \n",
       "        [[ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         ...,\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True]],\n",
       " \n",
       "        [[ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         ...,\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         ...,\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True]],\n",
       " \n",
       "        [[ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         ...,\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True]],\n",
       " \n",
       "        [[ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         ...,\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True]]]),\n",
       " array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True]),\n",
       " array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True]),\n",
       " array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True])]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[a == b for a, b in zip(model.self_attention.get_weights(), model.bert.bert.encoder.layer[-1].attention.get_weights())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-cased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# single tower english\n",
    "model_type = \"single_tower\"\n",
    "cased = True\n",
    "mask_sb = True\n",
    "\n",
    "\n",
    "init_dir = '../models/bert-base-cased'\n",
    "init_model = 'bert-base-cased'\n",
    "data_path = 'data/semeval2016/ABSA16_Restaurants_Train_SB1_v2.xml'\n",
    "checkpoint_path = \"./checkpoint/v0_256_1_30_cased_mask_b_avg_loss_aug_2/\"\n",
    "lang = \"en\"\n",
    "\n",
    "# checkpoint_path = \n",
    "# test_path = \"./data/semeval2016/EN_REST_SB1_TEST_LABELED.xml\"\n",
    "test_path = None\n",
    "tokenizer = AutoTokenizer.from_pretrained(init_model, cache_dir=init_dir)\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "# test_dataset = ChineseDataset(test_path, tokenizer, ASPECT_SENTENCE, mask_sb, model_type)\n",
    "\n",
    "trainer = ABSATrainer(\n",
    "    None,\n",
    "    data_path,\n",
    "    test_path,\n",
    "    logger=logging.getLogger(),\n",
    "    checkpoint=checkpoint_path,\n",
    "    cased=cased,\n",
    "    model_type=model_type,\n",
    "    mask_sb=mask_sb,\n",
    "    lang=lang\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n",
      "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from models.end_to_end_model import End2EndAspectSentimentModel\n",
    "model_checkpoint = \"checkpoint/ttee_pretrain_db_amazon_less_pos.csv_pretrain_60000steps_0d_2e-5lr_3000_0.96_schedule_0cased_update_0.2dropout/\"\n",
    "ckpt_path = tf.train.latest_checkpoint(model_checkpoint)\n",
    "config_path = os.path.join(model_checkpoint, \"model_config.json\")\n",
    "config = json.load(open(config_path))\n",
    "\n",
    "model = End2EndAspectSentimentModel.from_config(config)\n",
    "model.build_params()\n",
    "ckpt = tf.train.Checkpoint(model=model)\n",
    "ckpt.restore(ckpt_path)\n",
    "\n",
    "init_dir = \"bert_models/bert-base-uncased\"\n",
    "init_model = 'bert-base-uncased'\n",
    "mask_sb = True\n",
    "model_type = \"end_to_end\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(init_model, cache_dir=init_dir)\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "test_tokenizer = TestTokenizer(tokenizer, ASPECT_SENTENCE, mask_sb, model_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init_dir = \"bert_models/bert-base-uncased\"\n",
    "# init_model = 'bert-base-uncased'\n",
    "# mask_sb = True\n",
    "# model_type = \"end_to_end\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(init_model, cache_dir=init_dir)\n",
    "# tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "# test_tokenizer = TestTokenizer(tokenizer, ASPECT_SENTENCE, mask_sb, model_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'decoded_sequence': <tf.Tensor: shape=(1, 1, 17), dtype=int32, numpy=array([[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]], dtype=int32)>,\n",
       " 'output_logits': <tf.Tensor: shape=(1, 1, 17, 3), dtype=float32, numpy=\n",
       " array([[[[0.3339855 , 1.5488535 , 0.53060734],\n",
       "          [2.1150715 , 2.9006262 , 2.2540603 ],\n",
       "          [1.1242604 , 2.0321634 , 1.3172444 ],\n",
       "          [2.2598667 , 3.7435617 , 2.6251085 ],\n",
       "          [1.8312638 , 2.3520985 , 2.3375266 ],\n",
       "          [2.2344587 , 3.1792464 , 2.500115  ],\n",
       "          [2.1726778 , 3.4260092 , 2.6763983 ],\n",
       "          [2.0376005 , 3.9933152 , 1.9630762 ],\n",
       "          [2.3745065 , 3.7014742 , 2.2468858 ],\n",
       "          [1.897906  , 2.7479606 , 2.2566772 ],\n",
       "          [0.9903881 , 1.8187201 , 1.2670729 ],\n",
       "          [1.8719382 , 2.7461789 , 2.1032097 ],\n",
       "          [1.0907042 , 1.9613485 , 1.4420848 ],\n",
       "          [1.4756262 , 2.203     , 1.8667266 ],\n",
       "          [1.4946609 , 2.3378208 , 1.6724123 ],\n",
       "          [0.9005879 , 1.9585993 , 1.2643363 ],\n",
       "          [0.35056174, 1.2990711 , 0.75588906]]]], dtype=float32)>,\n",
       " 'output_cls_states': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[2.8641465]], dtype=float32)>,\n",
       " 'sim_matrix': <tf.Tensor: shape=(1, 1, 17, 1), dtype=float32, numpy=\n",
       " array([[[[0.04988611],\n",
       "          [0.08425312],\n",
       "          [0.03885285],\n",
       "          [0.11224487],\n",
       "          [0.0850076 ],\n",
       "          [0.10294122],\n",
       "          [0.12894312],\n",
       "          [0.11374516],\n",
       "          [0.09810883],\n",
       "          [0.08871009],\n",
       "          [0.03390899],\n",
       "          [0.08629744],\n",
       "          [0.03111301],\n",
       "          [0.05444309],\n",
       "          [0.04472494],\n",
       "          [0.01922287],\n",
       "          [0.11106602]]]], dtype=float32)>}"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = [\n",
    "    \"the actor is skillful, I like the scenario and story of the film\"\n",
    "]\n",
    "\n",
    "aspect_texts = [\n",
    "    [\"scenario positive\"],\n",
    "]\n",
    "data = test_tokenizer.tokenize(test_data, aspect_texts=aspect_texts)\n",
    "inputs = [data['input_ids'], data['token_type_ids'], data['attention_mask']]\n",
    "aspect_inputs = [data['aspect_input_ids'], data['aspect_token_type_ids'], data['aspect_attention_mask']]\n",
    "output = model(inputs, aspect_inputs, phase=\"dynamic_aspect_test\", output_attentions=True)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-cased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# end to end \n",
    "cased = False\n",
    "mask_sb = True\n",
    "model_type = \"end_to_end\"\n",
    "\n",
    "\n",
    "\n",
    "init_dir = \"bert_models/bert-base-uncased\"\n",
    "init_model = 'bert-base-uncased'\n",
    "data_path = 'data/semeval2016/ABSA16_Restaurants_Train_SB1_v2.xml'\n",
    "test_path = 'data/semeval2016/EN_REST_SB1_TEST_LABELED.xml'\n",
    "checkpoint_path = \"./checkpoint/v2_e2e_30_2_layer_pool_256_h_aug23/\"\n",
    "# checkpoint_path = \"./checkpoint/v2_e2e_30_2_layer_pool_256_h_aug23/\"\n",
    "# checkpoint_path = \"./checkpoint/v2_e2e_30_2_layer_pool_256_h_aug2_2e-5_cased_dropout/\"\n",
    "# checkpoint_path = \"./checkpoint/v2_e2e_50_2_layer_pool_256_h_2e-5_aug2_cased_update_dropout_0.1_drop_null_cache_aspsenti\"\n",
    "lang = \"en\"\n",
    "\n",
    "# init_dir = '../models/bert-base-chinese'\n",
    "# init_model = 'bert-base-chinese'\n",
    "# data_path = 'data/semeval2016/phone_chinese/labeled_phone.csv'\n",
    "# checkpoint_path = \"./checkpoint/cn_e2e_30_2_layer_pool_256_h_aug23\"\n",
    "# lang = \"cn\"\n",
    "# test_path = None\n",
    "\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(init_model, cache_dir=init_dir)\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "# test_dataset = ChineseDataset(test_path, tokenizer, ASPECT_SENTENCE, mask_sb, model_type)\n",
    "\n",
    "trainer = ABSATrainer(\n",
    "    None,\n",
    "    data_path,\n",
    "    test_path,\n",
    "    logger=logging.getLogger(),\n",
    "    checkpoint=checkpoint_path,\n",
    "    cased=cased,\n",
    "    model_type=model_type,\n",
    "    mask_sb=mask_sb,\n",
    "    lang=lang\n",
    ")\n",
    "# test_tokenizer = TestTokenizer(trainer.tokenizer, ASPECT_SENTENCE_CHINESE, mask_sb, model_type)\n",
    "test_tokenizer = TestTokenizer(trainer.tokenizer, ASPECT_SENTENCE, mask_sb, model_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cf = {\"init_bert_model\": \"bert-base-cased\", \"sentence_b\": {\"texts\": [\"general of ambience\", \"price of drinks\", \"quality of drinks\", \"style options of drinks\", \"prices of food\", \"quality of food\", \"style options of food\", \"general of location\", \"general of restaurant\", \"miscellaneous of restaurant\", \"prices of restaurant\", \"general of service\"], \"sentiments\": [\"negative\", \"neutral\", \"positive\"]}, \"num_sentiment_classes\": 3, \"subblock_hidden_size\": 256, \"subblock_head_num\": 1, \"cache_dir\": \"../models/bert-base-cased\", \"fuse_strategy\": \"concat\"}\n",
    "# trainer_2 = ABSATrainer(\n",
    "#     None,\n",
    "#     data_path,\n",
    "#     test_path,\n",
    "#     logger=logging.getLogger(),\n",
    "#     checkpoint=None,\n",
    "#     cased=cased,\n",
    "#     model_type=model_type,\n",
    "#     mask_sb=mask_sb,\n",
    "#     config=cf\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = [\n",
    "    \"Comfortable sofa and chairs, good hamburgers. But the waiter is very rude.\"\n",
    "]\n",
    "# data = test_tokenizer.tokenize(test_data)\n",
    "# inputs = [data['input_ids'], data['token_type_ids'], data['attention_mask']]\n",
    "# aspect_inputs = [data['aspect_input_ids'], data['aspect_token_type_ids'], data['aspect_attention_mask']]\n",
    "# trainer.model(inputs, aspect_inputs, phase=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{\"target\": \"chairs\", \"aspect\": \"AMBIENCE#GENERAL\", \"polarity\": \"positive\", \"prob\": 0.9267281889915466},\n",
       "  {\"target\": \"waiter\", \"aspect\": \"SERVICE#GENERAL\", \"polarity\": \"negative\", \"prob\": 0.9998960494995117},\n",
       "  {\"target\": \"sofa\", \"aspect\": \"AMBIENCE#GENERAL\", \"polarity\": \"positive\", \"prob\": 0.23541374504566193},\n",
       "  {\"target\": \"hamburgers\", \"aspect\": \"FOOD#QUALITY\", \"polarity\": \"positive\", \"prob\": 0.9992883205413818}]]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res, _ = quick_test(test_data, test_tokenizer, trainer, output_attentions=True)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:absl:Function `call` contains input name(s) text_inputs, aspect_inputs with unsupported characters which will be renamed to text_inputs_2, aspect_inputs_2 in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <models.end_to_end_model.End2EndAspectSentimentModel object at 0x7f262461ec70>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <models.end_to_end_model.End2EndAspectSentimentModel object at 0x7f262461ec70>, because it is not built.\n",
      "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:absl:Found untraced functions such as fuse_net_1_layer_call_fn, fuse_net_1_layer_call_and_return_conditional_losses, target_extraction_block_1_layer_call_fn, target_extraction_block_1_layer_call_and_return_conditional_losses, tf_bert_attention_1_layer_call_fn while saving (showing 5 of 1145). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./saved_models/en_aug23_cache/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./saved_models/en_aug23_cache/assets\n"
     ]
    }
   ],
   "source": [
    "signatures = {\n",
    "    \"text_inputs\": [tf.TensorSpec(shape=(None, None), dtype=tf.int32), tf.TensorSpec(shape=(None, None), dtype=tf.int32), tf.TensorSpec(shape=(None, None, None), dtype=tf.int32)],\n",
    "    \"aspect_inputs\": [tf.TensorSpec(shape=(None, None), dtype=tf.int32), tf.TensorSpec(shape=(None, None), dtype=tf.int32), tf.TensorSpec(shape=(None, None), dtype=tf.int32)]\n",
    "}\n",
    "test_func = trainer.model.call.get_concrete_function(text_inputs=signatures['text_inputs'], aspect_inputs=signatures['aspect_inputs'], label_inputs=None, phase=\"test\", output_attentions=False)\n",
    "tf.saved_model.save(trainer.model, \"./saved_models/en_aug23_cache\", signatures=test_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_data = test_tokenizer.tokenize([\"Good drinks\"])\n",
    "text_inputs = [tokenized_data['input_ids'], tokenized_data['token_type_ids'], tokenized_data['attention_mask']]\n",
    "aspect_inputs = [tokenized_data['aspect_input_ids'], tokenized_data['aspect_token_type_ids'], tokenized_data['aspect_attention_mask']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = infer_model.call(text_inputs=text_inputs, aspect_inputs=aspect_inputs, label_inputs=None, phase=\"test\", output_attentions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'origin_text': ['Comfortable sofa and chairs, good hamburgers. But the waiter is very rude.'],\n",
       " 'result': [[{'target': 'sofa',\n",
       "    'aspect': 'AMBIENCE#GENERAL',\n",
       "    'polarity': 'positive',\n",
       "    'prob': 0.23541486263275146},\n",
       "   {'target': 'hamburgers',\n",
       "    'aspect': 'FOOD#QUALITY',\n",
       "    'polarity': 'positive',\n",
       "    'prob': 0.9992883205413818},\n",
       "   {'target': 'waiter',\n",
       "    'aspect': 'SERVICE#GENERAL',\n",
       "    'polarity': 'negative',\n",
       "    'prob': 0.9998960494995117},\n",
       "   {'target': 'chairs',\n",
       "    'aspect': 'AMBIENCE#GENERAL',\n",
       "    'polarity': 'positive',\n",
       "    'prob': 0.9267282485961914}]],\n",
       " 'time_cost': 0.28431248664855957,\n",
       " 'status': 'success',\n",
       " 'error_code': 0}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "test_data = {\"text\": [\"Comfortable sofa and chairs, good hamburgers. But the waiter is very rude.\"],\n",
    "        \"language\": \"en\",\n",
    "        \"output_attentions\": False,\n",
    "}\n",
    "url = \"http://127.0.0.1:3030/absa_service\"\n",
    "# res = test_to|kenizer.tokenizer(test_data, padding=\"max_length\", max_length=64)\n",
    "res = requests.post(url, json=test_data)\n",
    "res.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "test_data = [\n",
    "    \"Comfortable sofa and chairs, good hamburgers. But the waiter is very rude.\"\n",
    "#     \"The hamburger taste good but is too expensive\",\n",
    "#     \"The hamburger tastes good but it is too expensive!\",\n",
    "]\n",
    "query_inputs = tokenizer.tokenize('[CLS]', add_special_tokens=False)\n",
    "key_inputs = tokenizer.tokenize(test_data[0], add_special_tokens=True)\n",
    "res, out = quick_test(test_data, test_tokenizer, trainer, output_attentions=True)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "at = res['sc_attention']\n",
    "at_list = []\n",
    "\n",
    "for i in range(at.size()):\n",
    "    at_list.append(at.read(i).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x216 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# % matplotlib inline\n",
    "query_inputs = tokenizer.tokenize('[CLS]', add_special_tokens=False)\n",
    "key_inputs = tokenizer.tokenize(test_data[0], add_special_tokens=True)\n",
    "attention = at_list[1].squeeze(0).squeeze(0)[:, :len(key_inputs)]\n",
    "df = pd.DataFrame(attention)\n",
    "df.columns = key_inputs\n",
    "df.index = query_inputs\n",
    "plt.figure(figsize=(10, 3))\n",
    "sns.heatmap(df, linewidths = 0.1, vmax=0.5, vmin=0, cmap=\"bwr\")\n",
    "plt.title(\"Sentiment Classification Module\")\n",
    "plt.tight_layout()\n",
    "# plt.savefig('./plots/bwr_ham_sc_attention.jpg', dpi=300)\n",
    "plt.show()\n",
    "# print(attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['origin_text', 'result', 'time_cost', 'status', 'attentions', 'error_code'])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from label_mappings import CATEGORY_LABEL_MAPPING_CHINESE, SENTIMENT_LABEL_MAPPING_CHINESE, CATEGORY_LABEL_MAPPING, SENTIMENT_LABEL_MAPPING\n",
    "matplotlib.rcParams['axes.unicode_minus']=False\n",
    "matplotlib.rcParams['font.sans-serif'] = ['FangSong']\n",
    "\n",
    "# % matplotlib inline\n",
    "# test_data = [\n",
    "# #     \"Comfortable sofa and chairs, good hamburgers. But the waiter is very rude.\"\n",
    "# #     \"The hamburger taste good but is too expensive\",\n",
    "# #     \"The hamburger tastes good but it is too expensive!\",\n",
    "#     \"\"\n",
    "# ]\n",
    "\n",
    "data = {\"text\": [\n",
    "        \"\"\n",
    "                ],\n",
    "        \"language\": \"en\",\n",
    "        \"output_attentions\": True,\n",
    "}\n",
    "\n",
    "query_inputs = tokenizer.tokenize('[CLS]', add_special_tokens=False)\n",
    "key_inputs = tokenizer.tokenize(test_data[\"text\"][0], add_special_tokens=True)\n",
    "\n",
    "res = requests.post(url, json=data).json()\n",
    "# res, attention = quick_test(test_data, test_tokenizer, trainer, output_attentions=True)\n",
    "# res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Length mismatch: Expected axis has 36 elements, new values have 33 elements",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-7b80b8eae31e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0msim_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'attentions'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#[np.array(asp_idxs)]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msim_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkey_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home1/liumiao/miniconda3/lib/python3.8/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m   5476\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5477\u001b[0m             \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5478\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5479\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5480\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/properties.pyx\u001b[0m in \u001b[0;36mpandas._libs.properties.AxisProperty.__set__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/home1/liumiao/miniconda3/lib/python3.8/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_set_axis\u001b[0;34m(self, axis, labels)\u001b[0m\n\u001b[1;32m    668\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_set_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 670\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    671\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_clear_item_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home1/liumiao/miniconda3/lib/python3.8/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mset_axis\u001b[0;34m(self, axis, new_labels)\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnew_len\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mold_len\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    221\u001b[0m                 \u001b[0;34mf\"Length mismatch: Expected axis has {old_len} elements, new \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m                 \u001b[0;34mf\"values have {new_len} elements\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Length mismatch: Expected axis has 36 elements, new values have 33 elements"
     ]
    }
   ],
   "source": [
    "asp_idxs = []\n",
    "query_inputs = []\n",
    "# for triplet in res[0]:\n",
    "#     asp_idx = CATEGORY_LABEL_MAPPING[triplet.aspect]\n",
    "#     senti_idx = SENTIMENT_LABEL_MAPPING[triplet.sentiment]\n",
    "#     asp_idxs.append(asp_idx * 3 + senti_idx)\n",
    "#     query_inputs.append(triplet.aspect + \"_\" + triplet.sentiment)\n",
    "for asp in CATEGORY_LABEL_MAPPING_CHINESE:\n",
    "    for senti in SENTIMENT_LABEL_MAPPING_CHINESE:\n",
    "        query_inputs.append(asp + \"_\" + senti)\n",
    "\n",
    "sim_matrix = np.array(res['attentions']).squeeze() #[np.array(asp_idxs)]\n",
    "df = pd.DataFrame(sim_matrix)\n",
    "df.index = query_inputs\n",
    "df.columns = key_inputs\n",
    "plt.figure(figsize=(20, 10))\n",
    "sns.heatmap(df, linewidths = 0.1, vmax=1, vmin=-1, cmap=\"bwr\")\n",
    "plt.title(\"Cosine Simlarity in FuseBlock\")\n",
    "plt.tight_layout()\n",
    "# plt.savefig('./plots/fusebock_sim_matrix_cn_all_3t.jpg', dpi=300)\n",
    "plt.show()\n",
    "# print(attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5 (default, Sep  4 2020, 07:30:14) \n[GCC 7.3.0]"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "ab1f4777d8eb8dfce46131a069ac753eeb3c9da22cbbba2710843f8e042f9b4a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
