{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "import logging\n",
    "import json\n",
    "import tensorflow as tf\n",
    "# gpus = tf.config.experimental.list_physical_devices(device_type='GPU')\n",
    "# tf.config.experimental.set_virtual_device_configuration(gpus[0],[tf.config.experimental.VirtualDeviceConfiguration(memory_limit=2048)])\n",
    "from train import ABSATrainer\n",
    "from my_datasets import EnglishDataset, TestTokenizer, ChineseDataset\n",
    "from transformers import AutoTokenizer, TFAutoModel\n",
    "from label_mappings import *\n",
    "from utils.eval_utils import quick_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7f8ac47c4460>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7f8ac47c4460>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f892c26bbb0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from models.end_to_end_model import End2EndAspectSentimentModel\n",
    "model_checkpoint = \"checkpoint/ttee_pretrain_20000steps_0d_2e-5lr_2000_0.96_schedule_0cased_gate_0.3dropout_1loss/\"\n",
    "ckpt_path = tf.train.latest_checkpoint(model_checkpoint)\n",
    "config_path = os.path.join(model_checkpoint, \"model_config.json\")\n",
    "config = json.load(open(config_path))\n",
    "\n",
    "model = End2EndAspectSentimentModel.from_config(config)\n",
    "model.build_params()\n",
    "ckpt = tf.train.Checkpoint(model=model)\n",
    "ckpt.restore(ckpt_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-42cfff251026>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_tokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTestTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mASPECT_SENTENCE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_sb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "test_tokenizer = TestTokenizer(tokenizer, ASPECT_SENTENCE, mask_sb, model_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"end2_end_aspect_sentiment_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " tf_bert_model (TFBertModel)  multiple                 109482240 \n",
      "                                                                 \n",
      " fuse_net (FuseNet)          multiple                  1181184   \n",
      "                                                                 \n",
      " target_extraction_block (Ta  multiple                 2322      \n",
      " rgetExtractionBlock)                                            \n",
      "                                                                 \n",
      " tf_bert_attention (TFBertAt  multiple                 2363904   \n",
      " tention)                                                        \n",
      "                                                                 \n",
      " sequential_1 (Sequential)   (36, 1)                   769       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 113,058,068\n",
      "Trainable params: 113,030,419\n",
      "Non-trainable params: 27,649\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.te_block.trainable = True\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-cased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from models.end_to_end_model import End2EndAspectSentimentModel\n",
    "\n",
    "init_dir = 'bert_models/bert-base-cased'\n",
    "init_model = 'bert-base-cased'\n",
    "# file_path = '../data/semeval2016/ABSA16_Restaurants_Train_SB1_v2.xml'\n",
    "# tokenizer = AutoTokenizer.from_pretrained('bert-base-cased', cache_dir=init_dir)\n",
    "# tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "# dataset = SemEvalDataSet(file_path, tokenizer, sentence_b=ASPECT_SENTENCE, model_type=\"end_to_end\")\n",
    "# ds = tf.data.Dataset.from_generator(\n",
    "#     dataset.generate_string_sample,\n",
    "#     output_types=(tf.string, tf.string)\n",
    "# ).batch(batch_size=8).map(dataset.wrap_map)\n",
    "# model = End2EndAspectSentimentModel(init_bert_model=init_model, sentence_b=ASPECT_SENTENCE, cache_dir=init_dir, hot_attention=True, fuse_strategy=\"gate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_inputs = tf.ones([1,10,768])\n",
    "model.self_attention(dummy_inputs, attention_mask=None, head_mask=None, output_attentions=False)\n",
    "model.self_attention.get_weights()\n",
    "model.self_attention.set_weights(model.bert.bert.encoder.layer[-1].attention.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[[ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         ...,\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True]],\n",
       " \n",
       "        [[ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         ...,\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True]],\n",
       " \n",
       "        [[ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         ...,\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         ...,\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True]],\n",
       " \n",
       "        [[ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         ...,\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True]],\n",
       " \n",
       "        [[ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         ...,\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True]]]),\n",
       " array([[ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True]]),\n",
       " array([[[ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         ...,\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True]],\n",
       " \n",
       "        [[ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         ...,\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True]],\n",
       " \n",
       "        [[ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         ...,\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         ...,\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True]],\n",
       " \n",
       "        [[ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         ...,\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True]],\n",
       " \n",
       "        [[ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         ...,\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True]]]),\n",
       " array([[ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True]]),\n",
       " array([[[ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         ...,\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True]],\n",
       " \n",
       "        [[ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         ...,\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True]],\n",
       " \n",
       "        [[ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         ...,\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         ...,\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True]],\n",
       " \n",
       "        [[ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         ...,\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True]],\n",
       " \n",
       "        [[ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         ...,\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True]]]),\n",
       " array([[ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True]]),\n",
       " array([[[ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         ...,\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True]],\n",
       " \n",
       "        [[ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         ...,\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True]],\n",
       " \n",
       "        [[ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         ...,\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         ...,\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True]],\n",
       " \n",
       "        [[ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         ...,\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True]],\n",
       " \n",
       "        [[ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         ...,\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True]]]),\n",
       " array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True]),\n",
       " array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True]),\n",
       " array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True])]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[a == b for a, b in zip(model.self_attention.get_weights(), model.bert.bert.encoder.layer[-1].attention.get_weights())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-cased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# single tower english\n",
    "model_type = \"single_tower\"\n",
    "cased = True\n",
    "mask_sb = True\n",
    "\n",
    "\n",
    "init_dir = '../models/bert-base-cased'\n",
    "init_model = 'bert-base-cased'\n",
    "data_path = 'data/semeval2016/ABSA16_Restaurants_Train_SB1_v2.xml'\n",
    "checkpoint_path = \"./checkpoint/v0_256_1_30_cased_mask_b_avg_loss_aug_2/\"\n",
    "lang = \"en\"\n",
    "\n",
    "# checkpoint_path = \n",
    "# test_path = \"./data/semeval2016/EN_REST_SB1_TEST_LABELED.xml\"\n",
    "test_path = None\n",
    "tokenizer = AutoTokenizer.from_pretrained(init_model, cache_dir=init_dir)\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "# test_dataset = ChineseDataset(test_path, tokenizer, ASPECT_SENTENCE, mask_sb, model_type)\n",
    "\n",
    "trainer = ABSATrainer(\n",
    "    None,\n",
    "    data_path,\n",
    "    test_path,\n",
    "    logger=logging.getLogger(),\n",
    "    checkpoint=checkpoint_path,\n",
    "    cased=cased,\n",
    "    model_type=model_type,\n",
    "    mask_sb=mask_sb,\n",
    "    lang=lang\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n",
      "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from models.end_to_end_model import End2EndAspectSentimentModel\n",
    "model_checkpoint = \"checkpoint/ttee_pretrain_db_amazon_less_pos.csv_pretrain_60000steps_0d_2e-5lr_3000_0.96_schedule_0cased_update_0.2dropout/\"\n",
    "ckpt_path = tf.train.latest_checkpoint(model_checkpoint)\n",
    "config_path = os.path.join(model_checkpoint, \"model_config.json\")\n",
    "config = json.load(open(config_path))\n",
    "\n",
    "model = End2EndAspectSentimentModel.from_config(config)\n",
    "model.build_params()\n",
    "ckpt = tf.train.Checkpoint(model=model)\n",
    "ckpt.restore(ckpt_path)\n",
    "\n",
    "init_dir = \"bert_models/bert-base-uncased\"\n",
    "init_model = 'bert-base-uncased'\n",
    "mask_sb = True\n",
    "model_type = \"end_to_end\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(init_model, cache_dir=init_dir)\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "test_tokenizer = TestTokenizer(tokenizer, ASPECT_SENTENCE, mask_sb, model_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init_dir = \"bert_models/bert-base-uncased\"\n",
    "# init_model = 'bert-base-uncased'\n",
    "# mask_sb = True\n",
    "# model_type = \"end_to_end\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(init_model, cache_dir=init_dir)\n",
    "# tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "# test_tokenizer = TestTokenizer(tokenizer, ASPECT_SENTENCE, mask_sb, model_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'decoded_sequence': <tf.Tensor: shape=(1, 1, 17), dtype=int32, numpy=array([[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]], dtype=int32)>,\n",
       " 'output_logits': <tf.Tensor: shape=(1, 1, 17, 3), dtype=float32, numpy=\n",
       " array([[[[0.3339855 , 1.5488535 , 0.53060734],\n",
       "          [2.1150715 , 2.9006262 , 2.2540603 ],\n",
       "          [1.1242604 , 2.0321634 , 1.3172444 ],\n",
       "          [2.2598667 , 3.7435617 , 2.6251085 ],\n",
       "          [1.8312638 , 2.3520985 , 2.3375266 ],\n",
       "          [2.2344587 , 3.1792464 , 2.500115  ],\n",
       "          [2.1726778 , 3.4260092 , 2.6763983 ],\n",
       "          [2.0376005 , 3.9933152 , 1.9630762 ],\n",
       "          [2.3745065 , 3.7014742 , 2.2468858 ],\n",
       "          [1.897906  , 2.7479606 , 2.2566772 ],\n",
       "          [0.9903881 , 1.8187201 , 1.2670729 ],\n",
       "          [1.8719382 , 2.7461789 , 2.1032097 ],\n",
       "          [1.0907042 , 1.9613485 , 1.4420848 ],\n",
       "          [1.4756262 , 2.203     , 1.8667266 ],\n",
       "          [1.4946609 , 2.3378208 , 1.6724123 ],\n",
       "          [0.9005879 , 1.9585993 , 1.2643363 ],\n",
       "          [0.35056174, 1.2990711 , 0.75588906]]]], dtype=float32)>,\n",
       " 'output_cls_states': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[2.8641465]], dtype=float32)>,\n",
       " 'sim_matrix': <tf.Tensor: shape=(1, 1, 17, 1), dtype=float32, numpy=\n",
       " array([[[[0.04988611],\n",
       "          [0.08425312],\n",
       "          [0.03885285],\n",
       "          [0.11224487],\n",
       "          [0.0850076 ],\n",
       "          [0.10294122],\n",
       "          [0.12894312],\n",
       "          [0.11374516],\n",
       "          [0.09810883],\n",
       "          [0.08871009],\n",
       "          [0.03390899],\n",
       "          [0.08629744],\n",
       "          [0.03111301],\n",
       "          [0.05444309],\n",
       "          [0.04472494],\n",
       "          [0.01922287],\n",
       "          [0.11106602]]]], dtype=float32)>}"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = [\n",
    "    \"the actor is skillful, I like the scenario and story of the film\"\n",
    "]\n",
    "\n",
    "aspect_texts = [\n",
    "    [\"scenario positive\"],\n",
    "]\n",
    "data = test_tokenizer.tokenize(test_data, aspect_texts=aspect_texts)\n",
    "inputs = [data['input_ids'], data['token_type_ids'], data['attention_mask']]\n",
    "aspect_inputs = [data['aspect_input_ids'], data['aspect_token_type_ids'], data['aspect_attention_mask']]\n",
    "output = model(inputs, aspect_inputs, phase=\"dynamic_aspect_test\", output_attentions=True)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-cased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# end to end \n",
    "cased = False\n",
    "mask_sb = True\n",
    "model_type = \"end_to_end\"\n",
    "\n",
    "\n",
    "\n",
    "init_dir = \"bert_models/bert-base-uncased\"\n",
    "init_model = 'bert-base-uncased'\n",
    "data_path = 'data/semeval2016/ABSA16_Restaurants_Train_SB1_v2.xml'\n",
    "test_path = 'data/semeval2016/EN_REST_SB1_TEST_LABELED.xml'\n",
    "checkpoint_path = \"./checkpoint/v2_e2e_30_2_layer_pool_256_h_aug23/\"\n",
    "# checkpoint_path = \"./checkpoint/v2_e2e_30_2_layer_pool_256_h_aug23/\"\n",
    "# checkpoint_path = \"./checkpoint/v2_e2e_30_2_layer_pool_256_h_aug2_2e-5_cased_dropout/\"\n",
    "# checkpoint_path = \"./checkpoint/v2_e2e_50_2_layer_pool_256_h_2e-5_aug2_cased_update_dropout_0.1_drop_null_cache_aspsenti\"\n",
    "lang = \"en\"\n",
    "\n",
    "# init_dir = '../models/bert-base-chinese'\n",
    "# init_model = 'bert-base-chinese'\n",
    "# data_path = 'data/semeval2016/phone_chinese/labeled_phone.csv'\n",
    "# checkpoint_path = \"./checkpoint/cn_e2e_30_2_layer_pool_256_h_aug23\"\n",
    "# lang = \"cn\"\n",
    "# test_path = None\n",
    "\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(init_model, cache_dir=init_dir)\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "# test_dataset = ChineseDataset(test_path, tokenizer, ASPECT_SENTENCE, mask_sb, model_type)\n",
    "\n",
    "trainer = ABSATrainer(\n",
    "    None,\n",
    "    data_path,\n",
    "    test_path,\n",
    "    logger=logging.getLogger(),\n",
    "    checkpoint=checkpoint_path,\n",
    "    cased=cased,\n",
    "    model_type=model_type,\n",
    "    mask_sb=mask_sb,\n",
    "    lang=lang\n",
    ")\n",
    "# test_tokenizer = TestTokenizer(trainer.tokenizer, ASPECT_SENTENCE_CHINESE, mask_sb, model_type)\n",
    "test_tokenizer = TestTokenizer(trainer.tokenizer, ASPECT_SENTENCE, mask_sb, model_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cf = {\"init_bert_model\": \"bert-base-cased\", \"sentence_b\": {\"texts\": [\"general of ambience\", \"price of drinks\", \"quality of drinks\", \"style options of drinks\", \"prices of food\", \"quality of food\", \"style options of food\", \"general of location\", \"general of restaurant\", \"miscellaneous of restaurant\", \"prices of restaurant\", \"general of service\"], \"sentiments\": [\"negative\", \"neutral\", \"positive\"]}, \"num_sentiment_classes\": 3, \"subblock_hidden_size\": 256, \"subblock_head_num\": 1, \"cache_dir\": \"../models/bert-base-cased\", \"fuse_strategy\": \"concat\"}\n",
    "# trainer_2 = ABSATrainer(\n",
    "#     None,\n",
    "#     data_path,\n",
    "#     test_path,\n",
    "#     logger=logging.getLogger(),\n",
    "#     checkpoint=None,\n",
    "#     cased=cased,\n",
    "#     model_type=model_type,\n",
    "#     mask_sb=mask_sb,\n",
    "#     config=cf\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = [\n",
    "    \"Comfortable sofa and chairs, good hamburgers. But the waiter is very rude.\"\n",
    "]\n",
    "# data = test_tokenizer.tokenize(test_data)\n",
    "# inputs = [data['input_ids'], data['token_type_ids'], data['attention_mask']]\n",
    "# aspect_inputs = [data['aspect_input_ids'], data['aspect_token_type_ids'], data['aspect_attention_mask']]\n",
    "# trainer.model(inputs, aspect_inputs, phase=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{\"target\": \"chairs\", \"aspect\": \"AMBIENCE#GENERAL\", \"polarity\": \"positive\", \"prob\": 0.9267281889915466},\n",
       "  {\"target\": \"waiter\", \"aspect\": \"SERVICE#GENERAL\", \"polarity\": \"negative\", \"prob\": 0.9998960494995117},\n",
       "  {\"target\": \"sofa\", \"aspect\": \"AMBIENCE#GENERAL\", \"polarity\": \"positive\", \"prob\": 0.23541374504566193},\n",
       "  {\"target\": \"hamburgers\", \"aspect\": \"FOOD#QUALITY\", \"polarity\": \"positive\", \"prob\": 0.9992883205413818}]]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res, _ = quick_test(test_data, test_tokenizer, trainer, output_attentions=True)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:absl:Function `call` contains input name(s) text_inputs, aspect_inputs with unsupported characters which will be renamed to text_inputs_2, aspect_inputs_2 in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <models.end_to_end_model.End2EndAspectSentimentModel object at 0x7f262461ec70>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <models.end_to_end_model.End2EndAspectSentimentModel object at 0x7f262461ec70>, because it is not built.\n",
      "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:absl:Found untraced functions such as fuse_net_1_layer_call_fn, fuse_net_1_layer_call_and_return_conditional_losses, target_extraction_block_1_layer_call_fn, target_extraction_block_1_layer_call_and_return_conditional_losses, tf_bert_attention_1_layer_call_fn while saving (showing 5 of 1145). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./saved_models/en_aug23_cache/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./saved_models/en_aug23_cache/assets\n"
     ]
    }
   ],
   "source": [
    "signatures = {\n",
    "    \"text_inputs\": [tf.TensorSpec(shape=(None, None), dtype=tf.int32), tf.TensorSpec(shape=(None, None), dtype=tf.int32), tf.TensorSpec(shape=(None, None, None), dtype=tf.int32)],\n",
    "    \"aspect_inputs\": [tf.TensorSpec(shape=(None, None), dtype=tf.int32), tf.TensorSpec(shape=(None, None), dtype=tf.int32), tf.TensorSpec(shape=(None, None), dtype=tf.int32)]\n",
    "}\n",
    "test_func = trainer.model.call.get_concrete_function(text_inputs=signatures['text_inputs'], aspect_inputs=signatures['aspect_inputs'], label_inputs=None, phase=\"test\", output_attentions=False)\n",
    "tf.saved_model.save(trainer.model, \"./saved_models/en_aug23_cache\", signatures=test_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_data = test_tokenizer.tokenize([\"Good drinks\"])\n",
    "text_inputs = [tokenized_data['input_ids'], tokenized_data['token_type_ids'], tokenized_data['attention_mask']]\n",
    "aspect_inputs = [tokenized_data['aspect_input_ids'], tokenized_data['aspect_token_type_ids'], tokenized_data['aspect_attention_mask']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = infer_model.call(text_inputs=text_inputs, aspect_inputs=aspect_inputs, label_inputs=None, phase=\"test\", output_attentions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'origin_text': ['Comfortable sofa and chairs, good hamburgers. But the waiter is very rude.'],\n",
       " 'result': [[{'target': 'sofa',\n",
       "    'aspect': 'AMBIENCE#GENERAL',\n",
       "    'polarity': 'positive',\n",
       "    'prob': 0.23541486263275146},\n",
       "   {'target': 'hamburgers',\n",
       "    'aspect': 'FOOD#QUALITY',\n",
       "    'polarity': 'positive',\n",
       "    'prob': 0.9992883205413818},\n",
       "   {'target': 'waiter',\n",
       "    'aspect': 'SERVICE#GENERAL',\n",
       "    'polarity': 'negative',\n",
       "    'prob': 0.9998960494995117},\n",
       "   {'target': 'chairs',\n",
       "    'aspect': 'AMBIENCE#GENERAL',\n",
       "    'polarity': 'positive',\n",
       "    'prob': 0.9267282485961914}]],\n",
       " 'time_cost': 0.28431248664855957,\n",
       " 'status': 'success',\n",
       " 'error_code': 0}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "test_data = {\"text\": [\"Comfortable sofa and chairs, good hamburgers. But the waiter is very rude.\"],\n",
    "        \"language\": \"en\",\n",
    "        \"output_attentions\": False,\n",
    "}\n",
    "url = \"http://127.0.0.1:3030/absa_service\"\n",
    "# res = test_to|kenizer.tokenizer(test_data, padding=\"max_length\", max_length=64)\n",
    "res = requests.post(url, json=test_data)\n",
    "res.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "test_data = [\n",
    "    \"Comfortable sofa and chairs, good hamburgers. But the waiter is very rude.\"\n",
    "#     \"The hamburger taste good but is too expensive\",\n",
    "#     \"The hamburger tastes good but it is too expensive!\",\n",
    "]\n",
    "query_inputs = tokenizer.tokenize('[CLS]', add_special_tokens=False)\n",
    "key_inputs = tokenizer.tokenize(test_data[0], add_special_tokens=True)\n",
    "res, out = quick_test(test_data, test_tokenizer, trainer, output_attentions=True)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "at = res['sc_attention']\n",
    "at_list = []\n",
    "\n",
    "for i in range(at.size()):\n",
    "    at_list.append(at.read(i).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x216 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# % matplotlib inline\n",
    "query_inputs = tokenizer.tokenize('[CLS]', add_special_tokens=False)\n",
    "key_inputs = tokenizer.tokenize(test_data[0], add_special_tokens=True)\n",
    "attention = at_list[1].squeeze(0).squeeze(0)[:, :len(key_inputs)]\n",
    "df = pd.DataFrame(attention)\n",
    "df.columns = key_inputs\n",
    "df.index = query_inputs\n",
    "plt.figure(figsize=(10, 3))\n",
    "sns.heatmap(df, linewidths = 0.1, vmax=0.5, vmin=0, cmap=\"bwr\")\n",
    "plt.title(\"Sentiment Classification Module\")\n",
    "plt.tight_layout()\n",
    "# plt.savefig('./plots/bwr_ham_sc_attention.jpg', dpi=300)\n",
    "plt.show()\n",
    "# print(attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['origin_text', 'result', 'time_cost', 'status', 'attentions', 'error_code'])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from label_mappings import CATEGORY_LABEL_MAPPING_CHINESE, SENTIMENT_LABEL_MAPPING_CHINESE, CATEGORY_LABEL_MAPPING, SENTIMENT_LABEL_MAPPING\n",
    "matplotlib.rcParams['axes.unicode_minus']=False\n",
    "matplotlib.rcParams['font.sans-serif'] = ['FangSong']\n",
    "\n",
    "# % matplotlib inline\n",
    "# test_data = [\n",
    "# #     \"Comfortable sofa and chairs, good hamburgers. But the waiter is very rude.\"\n",
    "# #     \"The hamburger taste good but is too expensive\",\n",
    "# #     \"The hamburger tastes good but it is too expensive!\",\n",
    "#     \"屏幕质量不错，很抗摔；电池续航很好，可以待机两天；相机不是很给，拍出来有点模糊\"\n",
    "# ]\n",
    "\n",
    "data = {\"text\": [\n",
    "        \"屏幕质量不错，很抗摔；电池续航很好，可以待机两天；相机不是很给，拍出来有点模糊\"\n",
    "                ],\n",
    "        \"language\": \"en\",\n",
    "        \"output_attentions\": True,\n",
    "}\n",
    "\n",
    "query_inputs = tokenizer.tokenize('[CLS]', add_special_tokens=False)\n",
    "key_inputs = tokenizer.tokenize(test_data[\"text\"][0], add_special_tokens=True)\n",
    "\n",
    "res = requests.post(url, json=data).json()\n",
    "# res, attention = quick_test(test_data, test_tokenizer, trainer, output_attentions=True)\n",
    "# res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Length mismatch: Expected axis has 36 elements, new values have 33 elements",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-7b80b8eae31e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0msim_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'attentions'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#[np.array(asp_idxs)]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msim_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkey_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home1/liumiao/miniconda3/lib/python3.8/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m   5476\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5477\u001b[0m             \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5478\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5479\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5480\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/properties.pyx\u001b[0m in \u001b[0;36mpandas._libs.properties.AxisProperty.__set__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/home1/liumiao/miniconda3/lib/python3.8/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_set_axis\u001b[0;34m(self, axis, labels)\u001b[0m\n\u001b[1;32m    668\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_set_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 670\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    671\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_clear_item_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home1/liumiao/miniconda3/lib/python3.8/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mset_axis\u001b[0;34m(self, axis, new_labels)\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnew_len\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mold_len\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    221\u001b[0m                 \u001b[0;34mf\"Length mismatch: Expected axis has {old_len} elements, new \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m                 \u001b[0;34mf\"values have {new_len} elements\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Length mismatch: Expected axis has 36 elements, new values have 33 elements"
     ]
    }
   ],
   "source": [
    "asp_idxs = []\n",
    "query_inputs = []\n",
    "# for triplet in res[0]:\n",
    "#     asp_idx = CATEGORY_LABEL_MAPPING[triplet.aspect]\n",
    "#     senti_idx = SENTIMENT_LABEL_MAPPING[triplet.sentiment]\n",
    "#     asp_idxs.append(asp_idx * 3 + senti_idx)\n",
    "#     query_inputs.append(triplet.aspect + \"_\" + triplet.sentiment)\n",
    "for asp in CATEGORY_LABEL_MAPPING_CHINESE:\n",
    "    for senti in SENTIMENT_LABEL_MAPPING_CHINESE:\n",
    "        query_inputs.append(asp + \"_\" + senti)\n",
    "\n",
    "sim_matrix = np.array(res['attentions']).squeeze() #[np.array(asp_idxs)]\n",
    "df = pd.DataFrame(sim_matrix)\n",
    "df.index = query_inputs\n",
    "df.columns = key_inputs\n",
    "plt.figure(figsize=(20, 10))\n",
    "sns.heatmap(df, linewidths = 0.1, vmax=1, vmin=-1, cmap=\"bwr\")\n",
    "plt.title(\"Cosine Simlarity in FuseBlock\")\n",
    "plt.tight_layout()\n",
    "# plt.savefig('./plots/fusebock_sim_matrix_cn_all_3t.jpg', dpi=300)\n",
    "plt.show()\n",
    "# print(attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5 (default, Sep  4 2020, 07:30:14) \n[GCC 7.3.0]"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "ab1f4777d8eb8dfce46131a069ac753eeb3c9da22cbbba2710843f8e042f9b4a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
